{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "from plotly import graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The logistic model aims to predict the discrete y variable a.k.a the target variable (e.g. whether something will happen) based on a collection of features. It does this by transforming a linear combination of the features into a curve and fitting this curve to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The curve used in logistic regression is the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define y as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} &= h_{\\boldsymbol{\\beta}}(\\mathbf{x})\\\\\n",
    "\\hat{y}&= \\sigma\\left(\\beta_0x_0+\\cdots+\\beta_nx_n\\right)\\quad &n\\in \\mathbb{N},x_0=1 \\\\\n",
    "\\hat{y}&=\\sigma\\left(\\sum^{n}_{i=0}\\beta_ix_i\\right) \\\\\n",
    "\\hat{y}&=\\sigma\\left(\\mathbf{\\boldsymbol{\\beta}^Tx}\\right)\\quad&\\boldsymbol{\\beta},\\mathbf{x}\\in\\mathbb{R}^{n\\times1}\\\\\n",
    "\\hat{y}&=\\sigma\\left(\\boldsymbol{\\beta}^T\\mathbf{x}\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\hat{y} = \\frac{1}{1+e^{-\\boldsymbol{\\beta}^T\\mathbf{x}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} + \\hat{y}e^{-\\boldsymbol{\\beta}^T\\mathbf{x}} &= 1\\\\\n",
    "\\hat{y}e^{-\\boldsymbol{\\beta}^T\\mathbf{x}} &= 1 - \\hat{y}\\\\\n",
    "\\frac{\\hat{y}}{1 - \\hat{y}} &= e^{\\boldsymbol{\\beta}^T\\mathbf{x}}\\\\\n",
    "\\ln\\left(\\frac{\\hat{y}}{1 - \\hat{y}}\\right)&=\\boldsymbol{\\beta}^T\\mathbf{x}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This above is the logit form of logistic regression. We model the logit as a linear combination of the x variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We define the cost function as follows for each y and corresponding x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "J(\\mathbf{x})\n",
    "&= \\begin{cases}\n",
    "-\\log\\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x})\\right) &\\text{if y=1}\\\\\n",
    "-\\log\\left(1-h_{\\boldsymbol{\\beta}}(\\mathbf{x})\\right) &\\text{if y=0}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "J(\\mathbf{x})\n",
    "&= -\\frac{1}{m}\\sum_{j=1}^my^j\\log\\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)\\right)\n",
    "+(1-y^j)\\log\\left(1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)\\right)\\\\\n",
    "&= -\\frac{1}{m}\\sum_{j=1}^my^j\\log\\left(\\frac{1}{1+e^{-\\boldsymbol{\\beta}^T\\mathbf{x}}}\\right)\n",
    "+(1-y^j)\\log\\left(1-\\frac{1}{1+e^{-\\boldsymbol{\\beta}^T\\mathbf{x}}}\\right)\\\\\n",
    "&= -\\frac{1}{m}\\sum_{j=1}^my^j\\log\\left(\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\\right)\n",
    "+(1-y^j)\\log\\left(1-\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)&=\\frac{1}{1+e^{-\\boldsymbol{\\beta}^T\\mathbf{x}^j}}\\\\\n",
    "&=\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial h}{\\partial \\beta_k} &= -\\left(1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}\\right)^{-2}e^{-\\sum^{n}_{i=0}\\beta_ix_i} (-x_k^j)\\\\\n",
    "&=\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\frac{-e^{-\\sum^{n}_{i=0}\\beta_ix_i} (-x_k^j)}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\\\\\n",
    "&=\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\frac{(1-1-e^{-\\sum^{n}_{i=0}\\beta_ix_i})(-x_k^j)}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\\\\\n",
    "&=\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\left(\n",
    "\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}-\n",
    "\\frac{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\right)(-x_k^j)\\\\\n",
    "&=\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\left(\n",
    "\\frac{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}-\n",
    "\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\right)(x_k^j)\\\\\n",
    "&=\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\left(\n",
    "1-\n",
    "\\frac{1}{1+e^{-\\sum^{n}_{i=0}\\beta_ix_i}}\n",
    "\\right)(x_k^j)\\\\\n",
    "&=h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)(1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j))x_k^j\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We need to differentiate the cost function i.e. find the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial\\beta_k}\\left(\\boldsymbol{\\beta}\\right) \n",
    "&=\\frac{\\partial}{\\partial\\beta_k}\\left(\n",
    "-\\frac{1}{m}\\sum_{j=1}^my^j\\log\\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)\\right)\n",
    "+(1-y^j)\\log\\left(1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)\\right)\n",
    "\\right)\\\\\n",
    "&=-\\frac{1}{m}\\sum_{j=1}^m\\frac{y^j}{h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)}\\frac{\\partial h}{\\partial \\beta_k}\n",
    "+\\frac{-(1-y^j)}{1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)}\\frac{\\partial h}{\\partial \\beta_k}\\\\\n",
    "&=-\\frac{1}{m}\\sum_{j=1}^m\\frac{y^j}{h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)}\n",
    "h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)(1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j))x_k^j\n",
    "+\\frac{-(1-y^j)}{1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)}\n",
    "h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)(1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j))x_k^j\\\\\n",
    "&=-\\frac{1}{m}\\sum_{j=1}^my^j(1-h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j))x_k^j\n",
    "-(1-y^j)\n",
    "h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)x_k^j\\\\\n",
    "&=\\frac{1}{m}\\sum_{j=1}^m\n",
    "\\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)-y^j\\right)x_k^j\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "hence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\boldsymbol{\\beta}} J\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "       \\frac{\\partial J}{\\partial\\beta_1} \\\\\n",
    "       \\vdots \\\\\n",
    "       \\frac{\\partial J}{\\partial\\beta_n}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "       \\frac{1}{m}\\sum_{j=1}^m\n",
    "            \\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)-y^j\\right)x_1^j\\\\\n",
    "       \\vdots \\\\\n",
    "       \\frac{1}{m}\\sum_{j=1}^m\n",
    "           \\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)-y^j\\right)x_n^j\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define the design matrix and column representation of y. Here each row of X and y are training examples hence there are m rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\mathbf{X}\\in\\mathbb{R}^{m\\times n},\n",
    "\\quad \\mathbf{y}\\in\\mathbb{R}^{m\\times 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\mathbf{X}=\\begin{bmatrix}\n",
    "       \\dots & (\\mathbf{x}^1)^T & \\dots\\\\\n",
    "       \\dots & (\\mathbf{x}^2)^T & \\dots\\\\\n",
    "       \\dots & \\vdots  & \\dots\\\\\n",
    "       \\dots & (\\mathbf{x}^m)^T & \\dots\n",
    "\\end{bmatrix}\\quad\n",
    "\\mathbf{y}=\\begin{bmatrix}\n",
    "    y_1\\\\y_2\\\\\\vdots\\\\y_m\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\beta}} J\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "       \\frac{1}{m}\\sum_{j=1}^m\n",
    "            \\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)-y^j\\right)x_1^j\\\\\n",
    "       \\vdots \\\\\n",
    "       \\frac{1}{m}\\sum_{j=1}^m\n",
    "           \\left(h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)-y^j\\right)x_n^j\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{m}\n",
    "\\begin{bmatrix}\n",
    "       \\sum^{n}_{i=0}h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)^jx^j_1\\\\\n",
    "       \\vdots \\\\\n",
    "       \\sum^{n}_{i=0}h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j)x^j_n\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\frac{1}{m}\n",
    "\\begin{bmatrix}\n",
    "       \\sum^{m}_{j=1}y^jx^j_1\\\\\n",
    "       \\vdots \\\\\n",
    "       \\sum^{m}_{j=1}y^jx^j_n\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "h_{\\boldsymbol{\\beta}}(\\mathbf{x}^j) = \\sigma({\\mathbf{x}^j}^T\\boldsymbol{\\beta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\beta}} J\n",
    "&=\\frac{1}{m}\\left(\n",
    "\\mathbf{X}^T\\sigma(\\mathbf{X}\\mathbf{\\boldsymbol{\\beta}})-\\mathbf{X}^T\\mathbf{y}\n",
    "\\right)\\\\\n",
    "&=\\frac{1}{m}\\mathbf{X}^T\\left(\n",
    "\\sigma(\\mathbf{X}\\mathbf{\\boldsymbol{\\beta}})-\\mathbf{y}\n",
    "\\right)\\\\\n",
    "&=\\frac{1}{m}\\mathbf{X}^T\\left(\n",
    "\\mathbf{\\hat{y}}-\\mathbf{y}\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\mathbf{\\hat{y}} = \\sigma(\\mathbf{X}\\mathbf{\\boldsymbol{\\beta}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We could have derived the same thing using matrix calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Example sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The curve used in logistic regression is the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e4341cd3ff4793acd7c0d968a8a355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': 'fc81bfbe-f9f8-419c-9923-4d127962d5e2',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid_fig = go.FigureWidget()\n",
    "demo_x = np.arange(-10,10,0.1)\n",
    "demo_y = 1 / (1 + np.exp(-demo_x))\n",
    "sigmoid_fig.add_scatter(\n",
    "    x=demo_x,\n",
    "    y=demo_y)\n",
    "sigmoid_fig.layout.title = 'Sigmoid Function'\n",
    "sigmoid_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Make fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "x0 = np.ones(shape=(m, 1))\n",
    "x1 = np.linspace(0, 10, m).reshape(-1, 1)\n",
    "X = np.column_stack((x0, x1))\n",
    "\n",
    "# let y = 0.5 * x + 1 + epsilon\n",
    "epsilon =  np.random.normal(scale=2, size=(m, 1))\n",
    "y = x1 + epsilon\n",
    "y = (y > 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e928c2fe5a1497889e2502722076d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'markers',\n",
       "              'name': 'linear data + noise',\n",
       "              'ty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget()\n",
    "fig = fig.add_scatter(\n",
    "    x=X[:,1],\n",
    "    y=y[:,0],\n",
    "    mode='markers',\n",
    "    name='linear data + noise')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Logistic regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, learning_rate=0.05):\n",
    "        \"\"\"  \n",
    "        Logistic regression model\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        learning_rate: float, optional, default 0.05\n",
    "            The learning rate parameter controlling the gradient descent\n",
    "            step size\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        print('Creating logistic model instance')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'<LogisticRegression '\n",
    "            f'learning_rate={self.learning_rate}>')\n",
    "\n",
    "    def fit(self, X, y, n_iter=1000):\n",
    "        \"\"\"  \n",
    "        Fit the logistic regression model\n",
    "\n",
    "        Updates the weights with n_iter iterations of batch gradient\n",
    "        descent updates\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, (n - 1) features + 1)\n",
    "            Note the first column of X is expected to be ones (to allow \n",
    "            for the bias to be included in beta)\n",
    "        y: numpy.ndarray\n",
    "            Target values - class label {0, 1}, shape (m samples, 1)\n",
    "        n_iter: int, optional, default 1000\n",
    "            Number of batch gradient descent steps\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        print(f'fitting with m={m} samples with n={n-1} features\\n')\n",
    "        self.beta = np.zeros(shape=(n, 1))\n",
    "        self.costs = []\n",
    "        self.betas = [self.beta]\n",
    "        for iteration in range(n_iter):\n",
    "            y_pred = self.predict_proba(X)\n",
    "            cost = (-1 / m) * (\n",
    "                (y.T @ np.log(y_pred)) +\n",
    "                ((np.ones(shape=y.shape) - y).T @ np.log(\n",
    "                    np.ones(shape=y_pred.shape) - y_pred))\n",
    "            )\n",
    "            self.costs.append(cost[0][0])\n",
    "            gradient = (1 / m) * X.T @ (y_pred - y)\n",
    "            self.beta = self.beta - (\n",
    "                self.learning_rate * gradient)\n",
    "            self.betas.append(self.beta)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"  \n",
    "        Predicted probability values for class 1\n",
    "\n",
    "        Note this is calculated as the sigmoid of the linear combination\n",
    "        of the feature values and the weights.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, (n - 1) features + 1)\n",
    "            Note the first column of X is expected to be ones (to allow \n",
    "            for the bias to be included in beta)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted probability of samples being in class 1\n",
    "        \"\"\"        \n",
    "        y_pred = self.sigmoid(X @ self.beta)\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, X, descision_prob=0.5):\n",
    "        \"\"\"  \n",
    "        Predict the class values from sample X feature values\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, (n - 1) features + 1)\n",
    "            Note the first column of X is expected to be ones (to allow \n",
    "            for the bias to be included in beta)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Prediceted class values, shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        y_pred = self.sigmoid(X @ self.beta)\n",
    "        return (y_pred > descision_prob) * 1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"  \n",
    "        Sigmoid function\n",
    "\n",
    "        f(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        x: numpy.ndarray\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            sigmoid of x, values in (0, 1)\n",
    "        \"\"\"        \n",
    "        return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating logistic model instance\n",
      "fitting with m=100 samples with n=1 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_X = np.array([[1,1],[1,4],[1,7]])\n",
    "logistic_regression.predict(example_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plot the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e928c2fe5a1497889e2502722076d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'markers',\n",
       "              'name': 'linear data + noise',\n",
       "              'ty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = fig.add_scatter(\n",
    "    x=X[:,1], \n",
    "    y=logistic_regression.predict_proba(X)[:,0],\n",
    "    mode='markers',\n",
    "    name='logistic best fit')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plot the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Haven't got round to this yet - see linear regression for an example error \n",
    "# surface decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logisitc regression - Titanic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_feather('../data/titanic/processed/X_train.feather')\n",
    "X_test = pd.read_feather('../data/titanic/processed/X_test.feather')\n",
    "y_train = pd.read_feather('../data/titanic/processed/y_train.feather')\n",
    "y_test = pd.read_feather('../data/titanic/processed/y_test.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating logistic model instance\n"
     ]
    }
   ],
   "source": [
    "titanic_logistic_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting with m=712 samples with n=29 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_logistic_model.fit(X=X_train.values, y=y_train.values, n_iter=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plot the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6486736c164288b3a79a20965d1168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': 'a3a5dfba-fddd-428e-87a9-4a7f4461bffc',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_cost_fig = go.FigureWidget()\n",
    "\n",
    "titanic_cost_fig.add_scatter(\n",
    "    x=list(range(len(titanic_logistic_model.costs))),\n",
    "    y=titanic_logistic_model.costs,\n",
    ")\n",
    "\n",
    "titanic_cost_fig.layout.title = 'Cost Vs gradient descent iterations'\n",
    "titanic_cost_fig.layout.xaxis.title = 'Iterations'\n",
    "titanic_cost_fig.layout.yaxis.title = 'Cost'\n",
    "titanic_cost_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is with my implementation 79.89%\n"
     ]
    }
   ],
   "source": [
    "y_pred = titanic_logistic_model.predict(X_test.values)\n",
    "test_accuracy = (y_pred == y_test.values).sum() / len(y_pred)\n",
    "\n",
    "print(f'Test accuracy is with my implementation {test_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
