{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "from plotly import graph_objects as go\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A random forest uses a collection of decision trees to predict data using the average values of the trees. \n",
    "\n",
    "The trees are trained on bootstrap samples (with replacement) of the same size as the input using a random subset of the features at each node split (normally sqrt(total number of features))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some variations of a random forest classifiers use majority voting however in my implementation I will take the class probabilities as the average across the trees in the random forest. Note this is the same as Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the regression setting the output of the random forest is simply the average of all the tree outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Define the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:decision_tree:New logger with name decision_tree\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger('decision_tree')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(f'New logger with name {logger.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class TreeNode():\n",
    "\n",
    "    count = itertools.count()\n",
    "\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 max_depth,\n",
    "                 min_samples_split,\n",
    "                 min_samples_leaf,\n",
    "                 n_classes=2,\n",
    "                 max_features=None,\n",
    "                 depth=0,\n",
    "                 impurity='gini',\n",
    "                 is_classifier=True):\n",
    "        \"\"\"\n",
    "        A single node in a decision tree\n",
    "\n",
    "        After recursive splitting of the input data, a given node \n",
    "        represents one split of the tree if it is not a leaf node. The\n",
    "        leaf node stores the training samples in that leaf to be used \n",
    "        for prediction. \n",
    "        The splitting nodes record the feature to split on as attribute \n",
    "        self.best_feature_index and the splitting value as attribute\n",
    "        self.best_feature_split_val\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features + 1 target)\n",
    "            Note the last column of the data are the target values\n",
    "        max_depth: int\n",
    "            The maximum depth allowed when \"growing\" a tree\n",
    "        min_samples_split: int\n",
    "            The minimum number of samples required to allow a split at\n",
    "            a the node\n",
    "        min_samples_leaf: int\n",
    "            The minimum number of samples allowed in a leaf. A split\n",
    "            candidate leading to less samples in a node than the\n",
    "            min_samples_leaf will be rejected\n",
    "        n_classes: int, optional, default 2\n",
    "            Number of classes in a classification setting. Ignored when\n",
    "            self.is_classifier = False\n",
    "        max_features: int, optional, default None\n",
    "            If set to 'sqrt' then only a random subset of features are\n",
    "            used to split at the node, the number of features used in\n",
    "            this case is sqrt(n_features).\n",
    "            Else all the features are considered when splitting at this\n",
    "            node\n",
    "        depth: int, optional, default 0\n",
    "            The depth of the node in the tree\n",
    "        impurity: str, optional, default 'gini'\n",
    "            The impurity measure to use when splitting at the node.\n",
    "            I have currently only implemented two\n",
    "            'gini' - Uses the gini impurity (for classification)\n",
    "            'mse' - Uses the mean square error - equal to variance (for\n",
    "            regression)\n",
    "        is_classifier: bool, optional, default True\n",
    "            Is the tree node used as part of a classification problem\n",
    "            or a regression problem. Should be set to True if\n",
    "            classification, False if regression\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = n_classes\n",
    "        self.max_features = max_features\n",
    "        self.depth = depth\n",
    "        self.impurity = impurity\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "        self.data_shape = data.shape\n",
    "        self.split_attempted = False\n",
    "        self.best_split_impurity = None\n",
    "        self.best_feature_index = None\n",
    "        self.best_feature_split_val = None\n",
    "        self.is_leaf = False\n",
    "        self.node_impurity = self.calculate_impurity([data[:, -1]])\n",
    "        self.value = self._init_value(data)\n",
    "        self.id = str(next(self.count))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'<TreeNode '\n",
    "            f'depth:{self.depth} '\n",
    "            f'node_impurity:{self.node_impurity:.2f} '\n",
    "            f'samples:{self.data_shape[0]} '\n",
    "            f'{\"ðŸŒ³\" if self.is_root else \"\"}'\n",
    "            f'{\"ðŸ\" if self.is_leaf else \"\"}'\n",
    "            f'>')\n",
    "\n",
    "    @property\n",
    "    def is_root(self):\n",
    "        return self.depth == 0\n",
    "\n",
    "    def info(self):\n",
    "        return dict(\n",
    "            data_shape=self.data_shape,\n",
    "            n_classes=self.n_classes,\n",
    "            depth=self.depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            node_impurity=self.node_impurity,\n",
    "            split_attempted=self.split_attempted,\n",
    "            best_split_impurity=self.best_split_impurity,\n",
    "            best_feature_index=self.best_feature_index,\n",
    "            best_feature_split_val=self.best_feature_split_val,\n",
    "            is_root=self.is_root)\n",
    "\n",
    "    def _init_value(self, data):\n",
    "        \"\"\"  \n",
    "        Returns the terminal node value based on the input data\n",
    "\n",
    "        For a classifier this is the class_counts.\n",
    "        For a regressor this is the average y value. \n",
    "\n",
    "        Note this value can be access at a splitting node to see what\n",
    "        the prediction would have been at that level of the tree\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features + 1 target)\n",
    "            Note the last column of the data are the target values\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray or float:\n",
    "            Class counts if classifier, else mean of target values \n",
    "        \"\"\"\n",
    "        if self.is_classifier:\n",
    "            return np.bincount(\n",
    "                data[:, -1].astype(int),\n",
    "                minlength=self.n_classes)\n",
    "        else:\n",
    "            return np.mean(data[:, -1])\n",
    "\n",
    "    def split(self, feature_index, feature_split_val, only_y=True):\n",
    "        \"\"\"  \n",
    "        Splits self.data on feature with index feature_index using\n",
    "        feature_split_val.\n",
    "\n",
    "        Each sample is included in left output if the feature value for\n",
    "        the sample is less than or equal to the feature_split_val else \n",
    "        it is included in the right output\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_index: int\n",
    "            Index of the feature (column) in self.data\n",
    "        feature_split_val: float\n",
    "            Feature value to use when splitting data\n",
    "        only_y: bool, optional, default True\n",
    "            Return only the y values in left and right - this is used \n",
    "            when checking candidate split purity increase\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        (numpy.ndarray, numpy.ndarray):\n",
    "            left and right splits of self.data\n",
    "        \"\"\"\n",
    "        assert feature_index in range(self.data.shape[1])\n",
    "        if only_y:\n",
    "            select = -1\n",
    "        else:\n",
    "            select = slice(None)\n",
    "        left_mask = self.data[:, feature_index] <= feature_split_val\n",
    "        right_mask = ~ left_mask\n",
    "        left = self.data[left_mask, select]\n",
    "        right = self.data[right_mask, select]\n",
    "        logger.debug(\n",
    "            f'Splitting on feature_index {feature_index} with '\n",
    "            f'feature_split_val = {feature_split_val} creates left '\n",
    "            f'with shape {left.shape} and right with '\n",
    "            f'shape {right.shape}')\n",
    "        return left, right\n",
    "\n",
    "    def gini_impurity(self, groups):\n",
    "        \"\"\"  \n",
    "        Calculate the Gini impurity for groups of values\n",
    "\n",
    "        The impurity returned is the weighted average of the impurity\n",
    "        of the groups.\n",
    "\n",
    "        You can think of gini impurity as the probability of incorrectly\n",
    "        predicting a random sample from a group if the prediction was\n",
    "        made based purely on the distribution of class labels in the\n",
    "        group\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        groups: tuple\n",
    "            The groups tuple is made up of arrays of values. It is \n",
    "            often called with groups = (left, right) to find the purity\n",
    "            of the candidate split\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Gini impurity\n",
    "        \"\"\"\n",
    "        gini = 0\n",
    "        total_samples = sum(group.shape[0] for group in groups)\n",
    "        for i, group in enumerate(groups):\n",
    "            group = group.astype(int)\n",
    "            class_counts = np.bincount(group, minlength=self.n_classes)\n",
    "            group_size = class_counts.sum()\n",
    "            class_probs = class_counts / group_size\n",
    "            unique_classes = np.count_nonzero(class_counts)\n",
    "            group_gini = (class_probs * (1 - class_probs)).sum()\n",
    "            gini += group_gini * (group_size / total_samples)\n",
    "            logger.debug(\n",
    "                f'Group {i} has size {group.shape[0]} with '\n",
    "                f'{unique_classes} unique classes '\n",
    "                f'with Gini index {group_gini:.3}')\n",
    "        return gini\n",
    "\n",
    "    def mean_square_impurity(self, groups):\n",
    "        \"\"\"  \n",
    "        Calculates the mean square error impurity\n",
    "\n",
    "        The mse impurity is the weighted average of the group variances\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        groups: tuple\n",
    "            The groups tuple is made up of arrays of values. It is \n",
    "            often called with groups = (left, right) to find the purity\n",
    "            of the candidate split\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Mean square error impurity\n",
    "        \"\"\"\n",
    "        mean_square_error = 0\n",
    "        total_samples = sum(group.shape[0] for group in groups)\n",
    "        for i, group in enumerate(groups):\n",
    "            group_size = group.shape[0]\n",
    "            group_mean = np.mean(group)\n",
    "            group_mean_square_error = np.mean((group - group_mean) ** 2)\n",
    "            mean_square_error += group_mean_square_error * \\\n",
    "                (group_size / total_samples)\n",
    "            logger.debug(\n",
    "                f'Group {i} has size {group.shape[0]} with '\n",
    "                f'with MSE impurity {group_mean_square_error:.3}')\n",
    "        logger.debug(f'MSE candidate {mean_square_error}')\n",
    "        return mean_square_error\n",
    "\n",
    "    def calculate_impurity(self, groups):\n",
    "        \"\"\"  \n",
    "        Calculates impurity based on self.impurity setting\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        groups: tuple\n",
    "            The groups tuple is made up of arrays of values. It is \n",
    "            often called with groups = (left, right) to find the purity\n",
    "            of the candidate split\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Mean square error of groups if self.impurity = 'mse'\n",
    "            Gini impurity of groups if self.impurity = 'mse'\n",
    "        \"\"\"\n",
    "        if self.impurity == 'gini':\n",
    "            return self.gini_impurity(groups)\n",
    "        elif self.impurity == 'mse':\n",
    "            return self.mean_square_impurity(groups)\n",
    "\n",
    "    def check_split(self, feature_index, feature_split_val):\n",
    "        \"\"\"  \n",
    "        Updates best split if candidate split is better\n",
    "\n",
    "        Splits the data in groups using self.split. Checks min samples\n",
    "        leaf condition after split. Calculates impurity of the split\n",
    "        then if impurity is less than best split already found and less\n",
    "        than the current node impurity the best_feature_index, the \n",
    "        best_feature_split_val and the best_split_impurity values are\n",
    "        updated.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_index: int\n",
    "            Index of the feature (column) in self.data\n",
    "        feature_split_val: float\n",
    "            Feature value to use when splitting data\n",
    "        \"\"\"\n",
    "        groups = self.split(feature_index, feature_split_val)\n",
    "        if any(len(group) < self.min_samples_leaf for group in groups):\n",
    "            logger.debug(\n",
    "                f\"Can't split node on feature {feature_index} with split \"\n",
    "                f\"val {feature_split_val} due to min_samples_leaf condition\")\n",
    "            return None\n",
    "        split_impurity = self.calculate_impurity(groups)\n",
    "        best_current_impurity = (\n",
    "            10**10 if self.best_split_impurity is None\n",
    "            else self.best_split_impurity)\n",
    "        if ((split_impurity < best_current_impurity) and\n",
    "                (split_impurity < self.node_impurity)):\n",
    "            logger.debug(\n",
    "                f'Found new best split with feature_split_val='\n",
    "                f'{feature_split_val} for feature_index = {feature_index} '\n",
    "                f'and split_impurity = {split_impurity:.2f}')\n",
    "            self.best_feature_index = feature_index\n",
    "            self.best_feature_split_val = feature_split_val\n",
    "            self.best_split_impurity = split_impurity\n",
    "\n",
    "    def find_best_split(self):\n",
    "        \"\"\"\n",
    "        Finds best split at the node\n",
    "\n",
    "        Loops through each feature and each unique value of that feature\n",
    "        checking for the best candidate split (i.e. the split that \n",
    "        reduces the impurity the most)\n",
    "\n",
    "        The function first checks if we have reached the max depth or if\n",
    "        self.data < self.min_samples_split. In either case no further\n",
    "        split is allowed and the function returns\n",
    "\n",
    "        All features are considered unless self.max_features == 'sqrt'\n",
    "        in which case a random subset of features are used of size\n",
    "        sqrt(n_features)\n",
    "        \"\"\"\n",
    "        if self.depth == self.max_depth:\n",
    "            return\n",
    "        if self.data.shape[0] < self.min_samples_split:\n",
    "            logger.info(f\"{self} can't split as samples < min_samples_split\")\n",
    "            return None\n",
    "        if self.node_impurity == 0:\n",
    "            logger.info(f\"Can't improve as node pure\")\n",
    "            return None\n",
    "        n_features = self.data.shape[1] - 1\n",
    "        all_feature_indices = np.arange(n_features)\n",
    "        if self.max_features == 'sqrt':\n",
    "            features_to_check = np.random.choice(\n",
    "                all_feature_indices,\n",
    "                size=np.sqrt(n_features).astype(int))\n",
    "        else:\n",
    "            features_to_check = all_feature_indices\n",
    "        logger.info(f'Checking features {features_to_check}')\n",
    "        for feature_index in features_to_check:\n",
    "            for feature_split_val in np.unique(self.data[:, feature_index]):\n",
    "                self.check_split(feature_index, feature_split_val)\n",
    "        self.split_attempted = True\n",
    "\n",
    "    def recursive_split(self):\n",
    "        \"\"\"  \n",
    "        Recursively grows tree by splitting to reduce impurity the most\n",
    "\n",
    "        The function finds the best split using the find_best_split\n",
    "        method. If there was a split found two nodes are created - left\n",
    "        and right. Finally the recursive_split method is called on each\n",
    "        of the new nodes.\n",
    "\n",
    "        Note the depth of the children node is incremented, otherwise\n",
    "        the node settings such as min_samples_split are passed to the\n",
    "        children nodes\n",
    "        \"\"\"\n",
    "        self.find_best_split()\n",
    "        if self.best_feature_index is not None:\n",
    "            logger.info(f'Splitting tree on feature_index '\n",
    "                        f'{self.best_feature_index} and feature_split_val '\n",
    "                        f'{self.best_feature_split_val:.2f}')\n",
    "            left, right = self.split(\n",
    "                feature_index=self.best_feature_index,\n",
    "                feature_split_val=self.best_feature_split_val,\n",
    "                only_y=False)\n",
    "            del self.data\n",
    "            self.left = TreeNode(\n",
    "                data=left,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                n_classes=self.n_classes,\n",
    "                max_features=self.max_features,\n",
    "                depth=self.depth + 1,\n",
    "                impurity=self.impurity,\n",
    "                is_classifier=self.is_classifier)\n",
    "            self.right = TreeNode(\n",
    "                data=right,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                n_classes=self.n_classes,\n",
    "                max_features=self.max_features,\n",
    "                depth=self.depth + 1,\n",
    "                impurity=self.impurity,\n",
    "                is_classifier=self.is_classifier)\n",
    "            self.left.recursive_split()\n",
    "            self.right.recursive_split()\n",
    "        else:\n",
    "            logger.info('Reached max depth or no splits reduce impurity')\n",
    "            self.is_leaf = True\n",
    "\n",
    "    def walk_depth_first(self, only_leaves=True):\n",
    "        \"\"\"  \n",
    "        Generator traversing of all nodes below and including this node\n",
    "\n",
    "        Depth first so visiting children before siblings\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        only_leaves: bool, optional, default True\n",
    "            Only return leaf nodes\n",
    "\n",
    "        Yields:\n",
    "            TreeNode: each node in tree\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            yield self\n",
    "        else:\n",
    "            if not only_leaves:\n",
    "                yield self\n",
    "            for node in (self.left, self.right):\n",
    "                yield from node.walk_depth_first(only_leaves)\n",
    "\n",
    "    def walk_breadth_first(self, layer=None):\n",
    "        \"\"\"  \n",
    "        Generator traversing of all nodes below and including this node\n",
    "\n",
    "        Breadth first so visiting siblings before children\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        only_leaves: bool, optional, default True\n",
    "            Only return leaf nodes\n",
    "\n",
    "        Yields:\n",
    "            TreeNode: each node in tree\n",
    "        \"\"\"\n",
    "        if layer is None:\n",
    "            layer = [self]\n",
    "        for node in layer:\n",
    "            yield node\n",
    "        new_layer = [\n",
    "            child\n",
    "            for node_children in [[node.left, node.right]\n",
    "                                  for node in layer if not node.is_leaf]\n",
    "            for child in node_children]\n",
    "        if new_layer:\n",
    "            yield from self.walk_breadth_first(new_layer)\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"  \n",
    "        prints ascii representation of tree below this node\n",
    "        \"\"\"\n",
    "        for node in self.walk_depth_first(only_leaves=False):\n",
    "            print('--' * node.depth + str(node))\n",
    "\n",
    "    def predict_row_proba(self, row):\n",
    "        \"\"\"\n",
    "        Predicts class probabilities for input row by walking the tree\n",
    "        and returning the leaf node class probabilities\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        row: numpy.ndarray\n",
    "            Input row, shape (n features,)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Class probabilities, shape (n classes, )\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            group_size = self.value.sum()\n",
    "            class_probs = self.value / group_size\n",
    "            return class_probs\n",
    "        elif row[self.best_feature_index] <= self.best_feature_split_val:\n",
    "            return self.left.predict_row_proba(row)\n",
    "        else:\n",
    "            return self.right.predict_row_proba(row)\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        \"\"\"Predicts class probabilities for input data\n",
    "\n",
    "        Predicts class probabilities for each row in data by walking the\n",
    "        tree and returning the leaf node class probabilities\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted sample class probabilities, \n",
    "            shape (m samples, n classes)\n",
    "        \"\"\"\n",
    "        if not self.is_classifier:\n",
    "            raise Exception('Not a classifier')\n",
    "        if len(data.shape) == 2:\n",
    "            return np.stack([self.predict_row_proba(row)\n",
    "                             for row in data])\n",
    "        else:\n",
    "            return self.predict_row_proba(data)\n",
    "\n",
    "    def predict_regressor_row(self, row):\n",
    "        \"\"\"\n",
    "        Predicts target value for input row by walking the tree\n",
    "        and returning the leaf node value\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        row: numpy.ndarray\n",
    "            Input row, shape (n features,)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Predicted target value\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            return self.value\n",
    "        elif row[self.best_feature_index] <= self.best_feature_split_val:\n",
    "            return self.left.predict_regressor_row(row)\n",
    "        else:\n",
    "            return self.right.predict_regressor_row(row)\n",
    "\n",
    "    def predict_regressor(self, data):\n",
    "        \"\"\"  \n",
    "        Predicts target values for each row in data by walking the\n",
    "        tree and returning the leaf node values\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values, shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        if len(data.shape) == 2:\n",
    "            return np.stack([self.predict_regressor_row(row)\n",
    "                             for row in data])\n",
    "        else:\n",
    "            return self.predict_regressor_row(data)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predicts target values or class labels for classification\n",
    "\n",
    "        Predicts target values/class for each row in data by walking the\n",
    "        tree and returning the leaf node value for regression or the \n",
    "        class with the largest predicted probability for classification\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values or class labels for classification\n",
    "        \"\"\"\n",
    "        if self.is_classifier:\n",
    "            return np.argmax(self.predict_proba(data), axis=-1)\n",
    "        else:\n",
    "            return self.predict_regressor(data)\n",
    "\n",
    "    def dot(self,\n",
    "            feature_names,\n",
    "            samples=True,\n",
    "            impurity=True,\n",
    "            value=True):\n",
    "        \"\"\"  \n",
    "        Returns Digraph visualizing the tree below this node\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_names: list[str]\n",
    "            List of feature names\n",
    "        samples: bool, optional, default True\n",
    "            Whether to display the number of samples on this node\n",
    "        impurity: bool, optional, default True\n",
    "            Whether to display the impurity value on this node\n",
    "        value: bool, optional, default True\n",
    "            Whether to dispaly the value on this node\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        graphviz.Digraph:\n",
    "            dot for tree diagram visual\n",
    "        \"\"\"\n",
    "        dot = Digraph(\n",
    "            comment='Decsion Tree',\n",
    "            node_attr=dict(shape=\"rectangle\",\n",
    "                           style=\"rounded\",\n",
    "                           fillcolor=\"#028d35\"))\n",
    "        for i, node in enumerate(self.walk_breadth_first()):\n",
    "            label = \"\"\n",
    "            if not node.is_leaf:\n",
    "                label += (\n",
    "                    f'{feature_names[node.best_feature_index]} <= '\n",
    "                    f'{node.best_feature_split_val}\\n')\n",
    "                dot.edge(node.id, node.left.id)\n",
    "                dot.edge(node.id, node.right.id)\n",
    "            if samples:\n",
    "                label += f'Samples = {node.data_shape[0]}\\n'\n",
    "            if impurity:\n",
    "                label += f'Impurity = {node.node_impurity:.2f}\\n'\n",
    "            if value:\n",
    "                if self.is_classifier:\n",
    "                    label += f'Class counts = {str(node.value)}\\n'\n",
    "                else:\n",
    "                    label += f'Average y = {node.value:.2f}\\n'\n",
    "            dot.node(name=node.id, label=label)\n",
    "        return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_depth=2,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 n_classes=2,\n",
    "                 max_features=None,\n",
    "                 impurity='gini',\n",
    "                 is_classifier=True):\n",
    "        \"\"\"Decision tree model\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        max_depth: int\n",
    "            The maximum depth allowed when \"growing\" a tree\n",
    "        min_samples_split: int\n",
    "            The minimum number of samples required to allow a split at\n",
    "            a the node\n",
    "        min_samples_leaf: int\n",
    "            The minimum number of samples allowed in a leaf. A split\n",
    "            candidate leading to less samples in a node than the\n",
    "            min_samples_leaf will be rejected\n",
    "        n_classes: int, optional, default 2\n",
    "            Number of classes in a classification setting. Ignored when\n",
    "            self.is_classifier = False\n",
    "        max_features: int, optional, default None\n",
    "            If set to 'sqrt' then only a random subset of features are\n",
    "            used to split at each node, the number of features used in\n",
    "            this case is sqrt(n_features).\n",
    "            Else all the features are considered when splitting at each\n",
    "            node\n",
    "        impurity: str, optional, default 'gini'\n",
    "            The impurity measure to use when splitting at each node.\n",
    "            I have currently only implemented two\n",
    "            'gini' - Uses the gini impurity (for classification)\n",
    "            'mse' - Uses the mean square error - equal to variance (for\n",
    "            regression)\n",
    "        is_classifier: bool, optional, default True\n",
    "            Is the model used as part of a classification problem\n",
    "            or a regression problem. Should be set to True if\n",
    "            classification, False if regression\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = n_classes\n",
    "        self.max_features = max_features\n",
    "        self.impurity = impurity\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "        self.is_fitted = False\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the decision tree model\n",
    "\n",
    "        The tree is fitted by instantiaing a root TreeNode instance and\n",
    "        then calling the recursive_split method. This iteratively grows\n",
    "        the tree by finding the best split to reduce the impurity the\n",
    "        most.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, n features)\n",
    "        y: numpy.ndarray\n",
    "            Target values, shape (m samples, n features)\n",
    "            If classifier with n_classes the values are assumed to be in\n",
    "            0, ..., n-1\n",
    "        \"\"\"\n",
    "        y_shape = (X.shape[0], 1)\n",
    "        data = np.concatenate((X, y.reshape(y_shape)), axis=1)\n",
    "        self.tree = TreeNode(\n",
    "            data=data,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            n_classes=self.n_classes,\n",
    "            max_features=self.max_features,\n",
    "            impurity=self.impurity,\n",
    "            is_classifier=self.is_classifier)\n",
    "        self.tree.recursive_split()\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predicts target values or class labels for classification\n",
    "\n",
    "        Predicts target values/class for each row in data by walking the\n",
    "        tree and returning the leaf node value for regression or the \n",
    "        class with the largest predicted probability for classification\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values or class labels for classification\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise Exception('Decision tree not fitted')\n",
    "        return self.tree.predict(data)\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        \"\"\"Predicts class probabilities for input data\n",
    "\n",
    "        Predicts class probabilities for each row in data by walking the\n",
    "        tree and returning the leaf node class probabilities\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted sample class probabilities, \n",
    "            shape (m samples, n classes)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise Exception('Decision tree not fitted')\n",
    "        return self.tree.predict_proba(data)\n",
    "\n",
    "    def render(self, feature_names):\n",
    "        \"\"\"Returns Digraph visualizing the decision tree (if fitted)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_names: list[str]\n",
    "            List of feature names\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        graphviz.Digraph:\n",
    "            dot for tree diagram visual\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            print('Decision tree not fitted')\n",
    "        else:\n",
    "            return self.tree.dot(feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class RandomForest():\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_depth=2,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 n_classes=2,\n",
    "                 max_features='sqrt',\n",
    "                 impurity='gini',\n",
    "                 is_classifier=True,\n",
    "                 n_trees=10,\n",
    "                 bootstrap=True):\n",
    "        \"\"\"Random forest model\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        max_depth: int\n",
    "            The maximum depth allowed when \"growing\" a tree\n",
    "        min_samples_split: int\n",
    "            The minimum number of samples required to allow a split at\n",
    "            a the node\n",
    "        min_samples_leaf: int\n",
    "            The minimum number of samples allowed in a leaf. A split\n",
    "            candidate leading to less samples in a node than the\n",
    "            min_samples_leaf will be rejected\n",
    "        n_classes: int, optional, default 2\n",
    "            Number of classes in a classification setting. Ignored when\n",
    "            self.is_classifier = False\n",
    "        max_features: int, optional, default None\n",
    "            If set to 'sqrt' then only a random subset of features are\n",
    "            used to split at each node, the number of features used in\n",
    "            this case is sqrt(n_features).\n",
    "            Else all the features are considered when splitting at each\n",
    "            node\n",
    "        impurity: str, optional, default 'gini'\n",
    "            The impurity measure to use when splitting at each node.\n",
    "            I have currently only implemented two\n",
    "            'gini' - Uses the gini impurity (for classification)\n",
    "            'mse' - Uses the mean square error - equal to variance (for\n",
    "            regression)\n",
    "        is_classifier: bool, optional, default True\n",
    "            Is the model used as part of a classification problem\n",
    "            or a regression problem. Should be set to True if\n",
    "            classification, False if regression\n",
    "        n_trees: int, optional, default 10\n",
    "            Number of trees in the forest\n",
    "        bootstrap: bool, optional, default True\n",
    "            Whether to bootstrap the data when fitting the trees\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = n_classes\n",
    "        self.max_features = max_features\n",
    "        self.impurity = impurity\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "        self.n_trees = n_trees\n",
    "        self.bootstrap = bootstrap\n",
    "        self.is_fitted = False\n",
    "        self.trees = []\n",
    "        np.random.seed(1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the random forest model\n",
    "\n",
    "        This method fits n_trees trees on the data with bootstrap\n",
    "        samples. A random subset of the features is used at each split.\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, n features)\n",
    "        y: numpy.ndarray\n",
    "            Target values, shape (m samples, 1)\n",
    "            If classifier with n_classes the values are assumed to be in\n",
    "            0, ..., n-1\n",
    "        \"\"\"\n",
    "        y_shape = (X.shape[0], 1)\n",
    "        data = np.concatenate((X, y.reshape(y_shape)), axis=1)\n",
    "        for i, data in enumerate(self._samples(data)):\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                n_classes=self.n_classes,\n",
    "                max_features=self.max_features,\n",
    "                impurity=self.impurity,\n",
    "                is_classifier=self.is_classifier)\n",
    "            logger.info(f'Fitting tree {i}')\n",
    "            tree.fit(X, y)\n",
    "            self.trees.append(tree)\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def _samples(self, data):\n",
    "        \"\"\"Bootstrap sample generator\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features + 1 target)\n",
    "            Note the last column of the data are the target values\n",
    "\n",
    "        Yields:\n",
    "            numpy.ndarray: Bootstrap sample of data\n",
    "        \"\"\"\n",
    "        n_rows = data.shape[0]\n",
    "        for _ in range(self.n_trees):\n",
    "            if not self.bootstrap:\n",
    "                yield data\n",
    "            else:\n",
    "                random_rows = np.random.choice(np.arange(n_rows),\n",
    "                                               size=n_rows,\n",
    "                                               replace=True)\n",
    "                yield data[random_rows, :]\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        \"\"\"Predicts class probabilities for input data\n",
    "\n",
    "        The class probability predictions from each tree are averaged to\n",
    "        provide the overall class prediction probabilities \n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted sample class probabilities, \n",
    "            shape (m samples, n classes)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise Exception('Forest not fitted')\n",
    "        # samples, classes, trees\n",
    "        return np.stack(list(tree.predict_proba(data) for tree in self.trees),\n",
    "                        axis=-1).sum(axis=-1) / self.n_trees\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predicts target values or class labels for classification\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values or class labels for classification\n",
    "        \"\"\"\n",
    "        if self.is_classifier:\n",
    "            return np.argmax(self.predict_proba(data), axis=-1)\n",
    "        else:\n",
    "            return np.stack(\n",
    "                list(tree.predict(data) for tree in self.trees),\n",
    "                axis=-1).mean(axis=-1)\n",
    "\n",
    "    def render(self, tree_id, feature_names):\n",
    "        \"\"\"Returns Digraph visualizing one of the decision trees\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        tree_id: [type]\n",
    "            tree index to display\n",
    "        feature_names: [type]\n",
    "            Feature names\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        graphviz.Digraph:\n",
    "            dot for tree diagram visual\n",
    "        \"\"\"\n",
    "        return self.trees[tree_id].render(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random forest classifier - Iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load the iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  y\n",
       "0                6.1               2.8                4.7               1.2  1\n",
       "1                5.7               3.8                1.7               0.3  0\n",
       "2                7.7               2.6                6.9               2.3  2\n",
       "3                6.0               2.9                4.5               1.5  1\n",
       "4                6.8               2.8                4.8               1.4  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data = load_iris()\n",
    "iris_df = pd.DataFrame(iris_data['data'],columns=iris_data['feature_names'])\n",
    "iris_df['y'] = iris_data['target']\n",
    "iris_df = iris_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "iris_sample = iris_df.head(5)\n",
    "iris_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit random forest classifier and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:decision_tree:Fitting tree 0\n",
      "INFO:decision_tree:Checking features [1 3]\n",
      "INFO:decision_tree:Splitting tree on feature_index 3 and feature_split_val 0.60\n",
      "INFO:decision_tree:Can't improve as node pure\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Checking features [1 1]\n",
      "INFO:decision_tree:Splitting tree on feature_index 1 and feature_split_val 2.40\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Fitting tree 1\n",
      "INFO:decision_tree:Checking features [0 0]\n",
      "INFO:decision_tree:Splitting tree on feature_index 0 and feature_split_val 5.40\n",
      "INFO:decision_tree:Checking features [0 2]\n",
      "INFO:decision_tree:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Checking features [1 2]\n",
      "INFO:decision_tree:Splitting tree on feature_index 2 and feature_split_val 4.70\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Fitting tree 2\n",
      "INFO:decision_tree:Checking features [1 2]\n",
      "INFO:decision_tree:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:decision_tree:Can't improve as node pure\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Checking features [2 2]\n",
      "INFO:decision_tree:Splitting tree on feature_index 2 and feature_split_val 4.70\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n",
      "INFO:decision_tree:Reached max depth or no splits reduce impurity\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"416pt\" height=\"258pt\"\n",
       " viewBox=\"0.00 0.00 416.36 258.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 254)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-254 412.36,-254 412.36,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M229.91,-250C229.91,-250 94.69,-250 94.69,-250 88.69,-250 82.69,-244 82.69,-238 82.69,-238 82.69,-198 82.69,-198 82.69,-192 88.69,-186 94.69,-186 94.69,-186 229.91,-186 229.91,-186 235.91,-186 241.91,-192 241.91,-198 241.91,-198 241.91,-238 241.91,-238 241.91,-244 235.91,-250 229.91,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.3\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal width (cm) &lt;= 0.6</text>\n",
       "<text text-anchor=\"middle\" x=\"162.3\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 150</text>\n",
       "<text text-anchor=\"middle\" x=\"162.3\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.67</text>\n",
       "<text text-anchor=\"middle\" x=\"162.3\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [50 50 50]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M140.41,-143C140.41,-143 12.19,-143 12.19,-143 6.19,-143 0.19,-137 0.19,-131 0.19,-131 0.19,-105 0.19,-105 0.19,-99 6.19,-93 12.19,-93 12.19,-93 140.41,-93 140.41,-93 146.41,-93 152.41,-99 152.41,-105 152.41,-105 152.41,-131 152.41,-131 152.41,-137 146.41,-143 140.41,-143\"/>\n",
       "<text text-anchor=\"middle\" x=\"76.3\" y=\"-127.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 50</text>\n",
       "<text text-anchor=\"middle\" x=\"76.3\" y=\"-113.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.00</text>\n",
       "<text text-anchor=\"middle\" x=\"76.3\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [50 &#160;0 &#160;0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.09,-185.99C125.26,-174.78 114.14,-162.12 104.24,-150.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.84,-148.5 97.62,-143.29 101.58,-153.11 106.84,-148.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M314.41,-150C314.41,-150 182.2,-150 182.2,-150 176.2,-150 170.2,-144 170.2,-138 170.2,-138 170.2,-98 170.2,-98 170.2,-92 176.2,-86 182.2,-86 182.2,-86 314.41,-86 314.41,-86 320.41,-86 326.41,-92 326.41,-98 326.41,-98 326.41,-138 326.41,-138 326.41,-144 320.41,-150 314.41,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"248.3\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">sepal width (cm) &lt;= 2.4</text>\n",
       "<text text-anchor=\"middle\" x=\"248.3\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"248.3\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.50</text>\n",
       "<text text-anchor=\"middle\" x=\"248.3\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [ 0 50 50]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.52,-185.99C197.36,-177.06 206.01,-167.19 214.22,-157.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217.01,-159.97 220.98,-150.14 211.75,-155.35 217.01,-159.97\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.41,-50C222.41,-50 108.19,-50 108.19,-50 102.19,-50 96.19,-44 96.19,-38 96.19,-38 96.19,-12 96.19,-12 96.19,-6 102.19,0 108.19,0 108.19,0 222.41,0 222.41,0 228.41,0 234.41,-6 234.41,-12 234.41,-12 234.41,-38 234.41,-38 234.41,-44 228.41,-50 222.41,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.3\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 10</text>\n",
       "<text text-anchor=\"middle\" x=\"165.3\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.18</text>\n",
       "<text text-anchor=\"middle\" x=\"165.3\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [0 9 1]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219.97,-85.94C211.61,-76.77 202.45,-66.72 194.06,-57.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.55,-55.06 187.22,-50.03 191.38,-59.78 196.55,-55.06\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M396.41,-50C396.41,-50 264.2,-50 264.2,-50 258.2,-50 252.2,-44 252.2,-38 252.2,-38 252.2,-12 252.2,-12 252.2,-6 258.2,0 264.2,0 264.2,0 396.41,0 396.41,0 402.41,0 408.41,-6 408.41,-12 408.41,-12 408.41,-38 408.41,-38 408.41,-44 402.41,-50 396.41,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"330.3\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 90</text>\n",
       "<text text-anchor=\"middle\" x=\"330.3\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.50</text>\n",
       "<text text-anchor=\"middle\" x=\"330.3\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [ 0 41 49]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276.3,-85.94C284.56,-76.77 293.61,-66.72 301.9,-57.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"304.56,-59.8 308.65,-50.03 299.36,-55.12 304.56,-59.8\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x12e933cd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_sample_vals = iris_sample.values\n",
    "\n",
    "# # for small sample\n",
    "# X = iris_sample_vals[:,:-1]\n",
    "# y = iris_sample_vals[:,-1]\n",
    "\n",
    "X = iris_df.values[:,:-1]\n",
    "y = iris_df.values[:,-1]\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "random_forest = RandomForest(n_classes=3, n_trees=3)\n",
    "random_forest.fit(X, y)\n",
    "\n",
    "feature_names = iris_data['feature_names']\n",
    "random_forest.render(tree_id=0, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Example prediction on Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03875969, 0.77235142, 0.18888889],\n",
       "       [0.70542636, 0.29457364, 0.        ],\n",
       "       [0.        , 0.22457912, 0.77542088],\n",
       "       [0.03875969, 0.77235142, 0.18888889]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.predict_proba(X[0:4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 0,\n",
       "       0, 2, 1, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 1,\n",
       "       1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2,\n",
       "       0, 2, 0, 2, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 1, 0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0,\n",
       "       0, 1, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1,\n",
       "       1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2,\n",
       "       0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random forest classifier - Titanic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load titanic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_feather('../data/titanic/processed/X_train.feather')\n",
    "y_train = pd.read_feather('../data/titanic/processed/y_train.feather')\n",
    "X_test = pd.read_feather('../data/titanic/processed/X_test.feather')\n",
    "y_test = pd.read_feather('../data/titanic/processed/y_test.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random forest model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 78.77%\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "titanic_forest = RandomForest(max_depth=2, min_samples_leaf=1, n_trees=50)\n",
    "titanic_forest.fit(X_train.values, y_train.values)\n",
    "y_pred = titanic_forest.predict(X_test.values)\n",
    "test_acc = (y_pred == y_test.values.flatten()).sum() / len(y_test)\n",
    "print(f'Test accuracy = {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"653pt\" height=\"258pt\"\n",
       " viewBox=\"0.00 0.00 653.36 258.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 254)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-254 649.36,-254 649.36,4 -4,4\"/>\n",
       "<!-- 38 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M390.41,-250C390.41,-250 258.2,-250 258.2,-250 252.2,-250 246.2,-244 246.2,-238 246.2,-238 246.2,-198 246.2,-198 246.2,-192 252.2,-186 258.2,-186 258.2,-186 390.41,-186 390.41,-186 396.41,-186 402.41,-192 402.41,-198 402.41,-198 402.41,-238 402.41,-238 402.41,-244 396.41,-250 390.41,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"324.3\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\">parch_mr &lt;= 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"324.3\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 712</text>\n",
       "<text text-anchor=\"middle\" x=\"324.3\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.47</text>\n",
       "<text text-anchor=\"middle\" x=\"324.3\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [444 268]</text>\n",
       "</g>\n",
       "<!-- 39 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>39</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M303.41,-150C303.41,-150 175.19,-150 175.19,-150 169.19,-150 163.19,-144 163.19,-138 163.19,-138 163.19,-98 163.19,-98 163.19,-92 169.19,-86 175.19,-86 175.19,-86 303.41,-86 303.41,-86 309.41,-86 315.41,-92 315.41,-98 315.41,-98 315.41,-138 315.41,-138 315.41,-144 309.41,-150 303.41,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.3\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">parch_mrs &lt;= 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"239.3\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 293</text>\n",
       "<text text-anchor=\"middle\" x=\"239.3\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.43</text>\n",
       "<text text-anchor=\"middle\" x=\"239.3\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [ 90 203]</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;39 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>38&#45;&gt;39</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M297.41,-185.99C289.66,-177.06 281.1,-167.19 272.99,-157.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275.51,-155.4 266.32,-150.14 270.22,-159.99 275.51,-155.4\"/>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M474.41,-150C474.41,-150 346.19,-150 346.19,-150 340.19,-150 334.19,-144 334.19,-138 334.19,-138 334.19,-98 334.19,-98 334.19,-92 340.19,-86 346.19,-86 346.19,-86 474.41,-86 474.41,-86 480.41,-86 486.41,-92 486.41,-98 486.41,-98 486.41,-138 486.41,-138 486.41,-144 480.41,-150 474.41,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"410.3\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">sex_c &lt;= 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"410.3\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 419</text>\n",
       "<text text-anchor=\"middle\" x=\"410.3\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.26</text>\n",
       "<text text-anchor=\"middle\" x=\"410.3\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [354 &#160;65]</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;40 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>38&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.52,-185.99C359.36,-177.06 368.01,-167.19 376.22,-157.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"379.01,-159.97 382.98,-150.14 373.75,-155.35 379.01,-159.97\"/>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>41</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M140.41,-50C140.41,-50 12.19,-50 12.19,-50 6.19,-50 0.19,-44 0.19,-38 0.19,-38 0.19,-12 0.19,-12 0.19,-6 6.19,0 12.19,0 12.19,0 140.41,0 140.41,0 146.41,0 152.41,-6 152.41,-12 152.41,-12 152.41,-38 152.41,-38 152.41,-44 146.41,-50 140.41,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"76.3\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 193</text>\n",
       "<text text-anchor=\"middle\" x=\"76.3\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.47</text>\n",
       "<text text-anchor=\"middle\" x=\"76.3\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [ 73 120]</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;41 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>39&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M183.67,-85.94C165.76,-75.94 145.99,-64.91 128.36,-55.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129.79,-51.85 119.35,-50.03 126.37,-57.96 129.79,-51.85\"/>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.41,-50C300.41,-50 182.2,-50 182.2,-50 176.2,-50 170.2,-44 170.2,-38 170.2,-38 170.2,-12 170.2,-12 170.2,-6 176.2,0 182.2,0 182.2,0 300.41,0 300.41,0 306.41,0 312.41,-6 312.41,-12 312.41,-12 312.41,-38 312.41,-38 312.41,-44 306.41,-50 300.41,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.3\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"241.3\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.28</text>\n",
       "<text text-anchor=\"middle\" x=\"241.3\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [17 83]</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;42 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>39&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.99,-85.94C240.17,-77.68 240.37,-68.72 240.55,-60.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.06,-60.1 240.78,-50.03 237.06,-59.95 244.06,-60.1\"/>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M473.41,-50C473.41,-50 345.19,-50 345.19,-50 339.19,-50 333.19,-44 333.19,-38 333.19,-38 333.19,-12 333.19,-12 333.19,-6 339.19,0 345.19,0 345.19,0 473.41,0 473.41,0 479.41,0 485.41,-6 485.41,-12 485.41,-12 485.41,-38 485.41,-38 485.41,-44 479.41,-50 473.41,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"409.3\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 352</text>\n",
       "<text text-anchor=\"middle\" x=\"409.3\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.24</text>\n",
       "<text text-anchor=\"middle\" x=\"409.3\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [304 &#160;48]</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;43 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>40&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M409.96,-85.94C409.87,-77.68 409.77,-68.72 409.68,-60.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"413.18,-59.99 409.57,-50.03 406.18,-60.07 413.18,-59.99\"/>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M633.41,-50C633.41,-50 515.2,-50 515.2,-50 509.2,-50 503.2,-44 503.2,-38 503.2,-38 503.2,-12 503.2,-12 503.2,-6 509.2,0 515.2,0 515.2,0 633.41,0 633.41,0 639.41,0 645.41,-6 645.41,-12 645.41,-12 645.41,-38 645.41,-38 645.41,-44 639.41,-50 633.41,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"574.3\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 67</text>\n",
       "<text text-anchor=\"middle\" x=\"574.3\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.38</text>\n",
       "<text text-anchor=\"middle\" x=\"574.3\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Class counts = [50 17]</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;44 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>40&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M466.29,-85.94C484.3,-75.94 504.19,-64.91 521.93,-55.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"523.95,-57.94 531,-50.03 520.56,-51.82 523.95,-57.94\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1328aae10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_forest.render(tree_id=3, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random forest regressor - Boston housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load Boston housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510</td>\n",
       "      <td>6.416</td>\n",
       "      <td>84.1</td>\n",
       "      <td>2.6463</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>395.50</td>\n",
       "      <td>9.04</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05644</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447</td>\n",
       "      <td>6.758</td>\n",
       "      <td>32.9</td>\n",
       "      <td>4.0776</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>3.53</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.09164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>6.065</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>390.91</td>\n",
       "      <td>5.52</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.09017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>6.297</td>\n",
       "      <td>91.8</td>\n",
       "      <td>2.3682</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>385.09</td>\n",
       "      <td>17.27</td>\n",
       "      <td>16.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "0  0.09178   0.0   4.05   0.0  0.510  6.416  84.1  2.6463   5.0  296.0   \n",
       "1  0.05644  40.0   6.41   1.0  0.447  6.758  32.9  4.0776   4.0  254.0   \n",
       "2  0.10574   0.0  27.74   0.0  0.609  5.983  98.8  1.8681   4.0  711.0   \n",
       "3  0.09164   0.0  10.81   0.0  0.413  6.065   7.8  5.2873   4.0  305.0   \n",
       "4  5.09017   0.0  18.10   0.0  0.713  6.297  91.8  2.3682  24.0  666.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT     y  \n",
       "0     16.6  395.50   9.04  23.6  \n",
       "1     17.6  396.90   3.53  32.4  \n",
       "2     20.1  390.11  18.07  13.6  \n",
       "3     19.2  390.91   5.52  22.8  \n",
       "4     20.2  385.09  17.27  16.1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data = load_boston()\n",
    "boston_df = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names'])\n",
    "boston_df['y'] = boston_data['target']\n",
    "boston_df = boston_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "boston_sample = boston_df.head(5)\n",
    "boston_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit random forest on Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"538pt\" height=\"258pt\"\n",
       " viewBox=\"0.00 0.00 538.28 258.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 254)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-254 534.28,-254 534.28,4 -4,4\"/>\n",
       "<!-- 363 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>363</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M312.42,-250C312.42,-250 216.86,-250 216.86,-250 210.86,-250 204.86,-244 204.86,-238 204.86,-238 204.86,-198 204.86,-198 204.86,-192 210.86,-186 216.86,-186 216.86,-186 312.42,-186 312.42,-186 318.42,-186 324.42,-192 324.42,-198 324.42,-198 324.42,-238 324.42,-238 324.42,-244 318.42,-250 312.42,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"264.64\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\">NOX &lt;= 0.668</text>\n",
       "<text text-anchor=\"middle\" x=\"264.64\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 404</text>\n",
       "<text text-anchor=\"middle\" x=\"264.64\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 89.96</text>\n",
       "<text text-anchor=\"middle\" x=\"264.64\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 22.94</text>\n",
       "</g>\n",
       "<!-- 364 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>364</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M244.42,-150C244.42,-150 148.86,-150 148.86,-150 142.86,-150 136.86,-144 136.86,-138 136.86,-138 136.86,-98 136.86,-98 136.86,-92 142.86,-86 148.86,-86 148.86,-86 244.42,-86 244.42,-86 250.42,-86 256.42,-92 256.42,-98 256.42,-98 256.42,-138 256.42,-138 256.42,-144 250.42,-150 244.42,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.64\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT &lt;= 5.39</text>\n",
       "<text text-anchor=\"middle\" x=\"196.64\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 327</text>\n",
       "<text text-anchor=\"middle\" x=\"196.64\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 81.66</text>\n",
       "<text text-anchor=\"middle\" x=\"196.64\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 25.08</text>\n",
       "</g>\n",
       "<!-- 363&#45;&gt;364 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>363&#45;&gt;364</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M243.12,-185.99C237.05,-177.23 230.35,-167.58 223.98,-158.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.82,-156.36 218.25,-150.14 221.07,-160.35 226.82,-156.36\"/>\n",
       "</g>\n",
       "<!-- 365 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>365</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.42,-150C381.42,-150 285.86,-150 285.86,-150 279.86,-150 273.86,-144 273.86,-138 273.86,-138 273.86,-98 273.86,-98 273.86,-92 279.86,-86 285.86,-86 285.86,-86 381.42,-86 381.42,-86 387.42,-86 393.42,-92 393.42,-98 393.42,-98 393.42,-138 393.42,-138 393.42,-144 387.42,-150 381.42,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"333.64\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">CHAS &lt;= 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"333.64\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 77</text>\n",
       "<text text-anchor=\"middle\" x=\"333.64\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 23.40</text>\n",
       "<text text-anchor=\"middle\" x=\"333.64\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 13.86</text>\n",
       "</g>\n",
       "<!-- 363&#45;&gt;365 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>363&#45;&gt;365</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M286.47,-185.99C292.64,-177.23 299.43,-167.58 305.9,-158.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308.82,-160.33 311.71,-150.14 303.09,-156.3 308.82,-160.33\"/>\n",
       "</g>\n",
       "<!-- 366 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>366</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.42,-50C107.42,-50 11.86,-50 11.86,-50 5.86,-50 -0.14,-44 -0.14,-38 -0.14,-38 -0.14,-12 -0.14,-12 -0.14,-6 5.86,0 11.86,0 11.86,0 107.42,0 107.42,0 113.42,0 119.42,-6 119.42,-12 119.42,-12 119.42,-38 119.42,-38 119.42,-44 113.42,-50 107.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.64\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 69</text>\n",
       "<text text-anchor=\"middle\" x=\"59.64\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 74.32</text>\n",
       "<text text-anchor=\"middle\" x=\"59.64\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 36.73</text>\n",
       "</g>\n",
       "<!-- 364&#45;&gt;366 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>364&#45;&gt;366</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M149.87,-85.94C135.1,-76.12 118.82,-65.31 104.21,-55.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.08,-52.65 95.82,-50.03 102.21,-58.48 106.08,-52.65\"/>\n",
       "</g>\n",
       "<!-- 367 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>367</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M244.42,-50C244.42,-50 148.86,-50 148.86,-50 142.86,-50 136.86,-44 136.86,-38 136.86,-38 136.86,-12 136.86,-12 136.86,-6 142.86,0 148.86,0 148.86,0 244.42,0 244.42,0 250.42,0 256.42,-6 256.42,-12 256.42,-12 256.42,-38 256.42,-38 256.42,-44 250.42,-50 244.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.64\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 258</text>\n",
       "<text text-anchor=\"middle\" x=\"196.64\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 37.60</text>\n",
       "<text text-anchor=\"middle\" x=\"196.64\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 21.96</text>\n",
       "</g>\n",
       "<!-- 364&#45;&gt;367 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>364&#45;&gt;367</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.64,-85.94C196.64,-77.68 196.64,-68.72 196.64,-60.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"200.14,-60.03 196.64,-50.03 193.14,-60.03 200.14,-60.03\"/>\n",
       "</g>\n",
       "<!-- 368 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M381.42,-50C381.42,-50 285.86,-50 285.86,-50 279.86,-50 273.86,-44 273.86,-38 273.86,-38 273.86,-12 273.86,-12 273.86,-6 279.86,0 285.86,0 285.86,0 381.42,0 381.42,0 387.42,0 393.42,-6 393.42,-12 393.42,-12 393.42,-38 393.42,-38 393.42,-44 387.42,-50 381.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"333.64\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 70</text>\n",
       "<text text-anchor=\"middle\" x=\"333.64\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 22.43</text>\n",
       "<text text-anchor=\"middle\" x=\"333.64\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 13.42</text>\n",
       "</g>\n",
       "<!-- 365&#45;&gt;368 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>365&#45;&gt;368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M333.64,-85.94C333.64,-77.68 333.64,-68.72 333.64,-60.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"337.14,-60.03 333.64,-50.03 330.14,-60.03 337.14,-60.03\"/>\n",
       "</g>\n",
       "<!-- 369 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>369</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M518.42,-50C518.42,-50 422.86,-50 422.86,-50 416.86,-50 410.86,-44 410.86,-38 410.86,-38 410.86,-12 410.86,-12 410.86,-6 416.86,0 422.86,0 422.86,0 518.42,0 518.42,0 524.42,0 530.42,-6 530.42,-12 530.42,-12 530.42,-38 530.42,-38 530.42,-44 524.42,-50 518.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"470.64\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 7</text>\n",
       "<text text-anchor=\"middle\" x=\"470.64\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 12.34</text>\n",
       "<text text-anchor=\"middle\" x=\"470.64\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 18.20</text>\n",
       "</g>\n",
       "<!-- 365&#45;&gt;369 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>365&#45;&gt;369</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M380.4,-85.94C395.18,-76.12 411.46,-65.31 426.07,-55.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"428.07,-58.48 434.46,-50.03 424.2,-52.65 428.07,-58.48\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1328aa190>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = boston_df.values[:,:-1]\n",
    "y = boston_df.values[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "boston_random_forest = RandomForest(\n",
    "    impurity='mse',\n",
    "    is_classifier=False,\n",
    "    n_trees=100,\n",
    "    max_depth=2)\n",
    "boston_random_forest.fit(X_train, y_train)\n",
    "\n",
    "boston_feature_names = boston_data['feature_names']\n",
    "boston_random_forest.render(tree_id=0,feature_names=boston_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random forest accuracy on Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (R2 score) = 61.39%\n"
     ]
    }
   ],
   "source": [
    "y_pred = boston_random_forest.predict(X_test)\n",
    "test_acc = r2_score(y_test, y_pred)\n",
    "print(f'Test accuracy (R2 score) = {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
