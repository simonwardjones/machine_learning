{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "from plotly import graph_objects as go\n",
    "from scipy.special import logsumexp, expit\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the gradient boosted decision tree is to find a model $y=F(x)$ to minimise a loss function $\\mathcal{L}$. Suppose we define the model iteratively such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}_i=F_m(x_i) \\quad\\forall i \\in {1,\\dots,n}\n",
    "$$\n",
    "Where n is the number of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As normal in an iterative model we want to refine the model to decrease the loss. ( In a parametic setting e.g. logistic regression we think of this as a weight update, stepping in the direction of negative gradient). \n",
    "In the gradient boosted decision tree setting, instead of revising $F_{m-1}$ directly we add another \"delta model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}_i = F_m(x_i) + f_{m+1}(x_i) \\quad\\forall i \\in {1,\\dots,n} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we choose $f_{m+1}(x_i)$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think about gradient descent ideally we would update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}_i \\to \\hat{y}_i - \\eta\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\eta$ is the learning rate, so we would ideally set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_{m+1}(x_i) = -\\eta\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise we cannot set this delta model exactly so we train a decision tree on data $\\left(x_i,-\\eta\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}_i}}\\right) \\forall i$  \n",
    "Note this will have it's own Loss $\\mathcal{L}^{f_{m+1}}$ separate to the global loss $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Point**\n",
    ">The gradient boosted decision tree is not trained on the residuals at each step. Rather it is trained on the negative gradient of the loss function evaluated using the prediction of the current step - which in the case of mean square error happens to be the residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of regression we define the loss function as the mean square error\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}) = \\frac{1}{2n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "hence\n",
    "$$\n",
    "-\\eta\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}_i}} = \\frac{\\eta}{n}(y_i-\\hat{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building fitting the tree we will ignore the 1/n as part of eta, the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the process looks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit $f_0(x)\\sim y$ then $F_0(x) = f_0(x)$  \n",
    "We fit $f_1(x)\\sim (y-F_0(x))$ then $F_1(x) = F_0(x) + \\eta f_1(x)$  \n",
    "We fit $f_2(x)\\sim (y-F_1(x))$ then $F_2(x) = F_1(x) + \\eta f_2(x)$  \n",
    "We fit $f_3(x)\\sim (y-F_2(x))$ then $F_3(x) = F_2(x) + \\eta f_3(x)$  \n",
    "...  \n",
    "We fit $f_m(x)\\sim (y-F_{m-1}(x))$ then $F_m(x) = F_{m-1}(x) + \\eta f_m(x)$\n",
    "\n",
    "Then predictions $\\hat{y} = F_m(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binomial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our iterative model was $\\hat{y}_i = F_m(x_i)$ where the $\\hat{y}_i$ directly represented the probability $x_i$ is in class 1. i.e. $P(x_i \\in C_1)$ where $C_1$ represents class 1.  \n",
    "\n",
    "Then the delta model doesn't make sense as we would be directly adding to a probability value. As in logistic regression it is often the case to fit the model to a transformation of probability.\n",
    "\n",
    "We define a model \n",
    "$$\n",
    "\\hat{y}\\sim F(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "$$\n",
    "\\hat{p} = \\frac{1}{1+e^{-\\hat{y}}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so \n",
    "$$\n",
    "\\hat{y} = \\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{p}$ represents the probability of being in class 1, $\\hat{y}$ is sometimes known as the logit.\n",
    "\n",
    "Note $\\hat{p}\\in[0,1],\\quad \\hat{y}\\in(-\\infty,\\infty),\\quad y\\in\\{0,1\\}$\n",
    "\n",
    "With this set up the model F is additive. Hence in the classification setting the gradient boosted decision tree predicts $\\hat{y}$ as a sum of multiple delta models. The probability values are then calculated by transforming $\\hat{y}$ using the sigmoid function (a.k.a the expit function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following fact later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{p} &= \\frac{1}{1+e^{-\\hat{y}}} \\quad so \\\\\n",
    "\\hat{p} &= \\frac{e^{\\hat{y}}}{e^{\\hat{y}}+1} \\quad so \\\\\n",
    "1 - \\hat{p} &= \\frac{e^{\\hat{y}}+1 -e^{\\hat{y}}}{e^{\\hat{y}}+1} \\quad so \\\\\n",
    "\\log\\left(1 - \\hat{p}\\right) &= -\\log\\left(e^{\\hat{y}}+1\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of classification we define the loss function as cross entropy\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\hat{y}) &= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "    -y_i\\log(\\hat{p}_i)-(1-y_i)\\log(1-\\hat{p}_i)\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "    -y_i\\log(\\hat{p}_i)+y_i\\log(1-\\hat{p}_i)-\\log(1-\\hat{p}_i)\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "    -y_i\\log(\\frac{\\hat{p}_i}{1-\\hat{p}_i})-\\log(1-\\hat{p}_i)\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "    -y_i\\hat{y}_i+\\log\\left(e^{\\hat{y}}+1\\right)\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to choose $f_{m+1}$ we fit to $-\\eta\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}_i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}_i}}\n",
    "= \\frac{1}{n}\\left(-y_i + \\frac{e^{\\hat{y}}}{e^{\\hat{y}}+1} \\right)\n",
    "= \\frac{1}{n}\\left(\\hat{p}_i - y_i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}_i}}\n",
    "= \\frac{1}{n}\\left(y_i - \\hat{p}_i \\right)\n",
    "= \\frac{1}{n}\\left(y_i - \\sigma(\\hat{y}_i) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\sigma$ is the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the process looks:\n",
    "\n",
    "we define initial values\n",
    "$f_0(x) = \\log\\left(\\frac{\\sum{y_i\\in C_1}}{\\sum{y_i\\notin C_1}}\\right)$ then set $F_0(x) = f_0(x)$  \n",
    "We fit $f_1(x)\\sim (y-\\sigma(F_0(x)))$ then $F_1(x) = F_0(x) + \\eta f_1(x)$  \n",
    "We fit $f_2(x)\\sim (y-\\sigma(F_1(x)))$ then $F_2(x) = F_1(x) + \\eta f_2(x)$  \n",
    "We fit $f_3(x)\\sim (y-\\sigma(F_2(x)))$ then $F_3(x) = F_2(x) + \\eta f_3(x)$  \n",
    "$\\vdots$   \n",
    "We fit $f_m(x)\\sim (y-\\sigma(F_{m-1}(x)))$ then $F_m(x) = F_{m-1}(x) + \\eta f_m(x)$\n",
    "\n",
    "Then predictions $\\hat{p} = \\sigma(F_m(x)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi class classification where K is equal to the number of classes we have to define the model set up a little differently. We model the log of each class probability as as an additive model.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log(\\hat{p}^1) &\\sim F^1(x) = \\hat{y}^1\\\\\n",
    "\\log(\\hat{p}^2) &\\sim F^2(x) = \\hat{y}^2\\\\\n",
    "&\\vdots\\\\\n",
    "\\log(\\hat{p}^K) &\\sim F^K(x) = \\hat{y}^K\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and we define  \n",
    "$$\n",
    "\\hat{p}^k = \\frac{e^{F^k(x)}}{\\sum_{j=1}^{K}e^{F^j(x)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we use Cross entropy but for multi class setting\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(F^1,\\dots,F^K) \n",
    "= -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K}\\mathbb{1}(y_i\\in C^k)\\log(\\hat{p}^k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $y^k$ be the indicator variable for class k, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(F^1,\\dots,F^K) \n",
    "&= -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K}y^k_i\\log\\left(\n",
    "\\frac{e^{F^k(x_i)}}{\\sum_{j=1}^{K}e^{F^j(x_i)}}\\right)\\\\\n",
    "&= -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K}y^k_i\\log\\left(\n",
    "\\frac{e^{\\hat{y}_i^k}}{\\sum_{j=1}^{K}e^{\\hat{y}_i^j}}\\right)\\\\\n",
    "&= -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K}\\left(\n",
    "y^k_i\\log\\left(e^{\\hat{y}^k_i}\\right)\n",
    "- y^k_i log\\left(\\sum_{j=1}^{K}e^{\\hat{y}^j_i}\\right)\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\n",
    "log\\left(\\sum_{j=1}^{K}e^{\\hat{y}^j_i}\\right)\n",
    " - \\sum_{k=1}^{K}y^k_i\\hat{y}^k_i\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{\\partial\\mathcal{L}}{\\partial{\\hat{y}^j_i}}\n",
    "&= \\frac{1}{n}\\left( y^j_i - \\frac{e^{\\hat{y}^j_i}}{\\sum_{l=1}^{K}e^{\\hat{y}^l_i}}\n",
    "\\right)\\\\\n",
    "&= \\frac{1}{n}\\left( y^j_i - \\hat{p}^j_i\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the process looks:\n",
    "\n",
    "We define initial values\n",
    "$f_0(x)^k = \\log\\left(\\frac{\\sum{y_i\\in C_k}}{\\sum{y_i\\notin C_k}}\\right)$ then set $F_0(x)^k = f_0(x)^k \\quad \\forall k \\in \\{1 \\dots K\\}$  \n",
    "We fit $f_1(x)^k\\sim (y^k-\\sigma_k(F_0^1,\\dots,F_0^K))$ then $F_1(x)^k = F_0(x)^k + \\eta f_1(x)^k \\quad\\forall k\\in\\{1,\\dots, K\\}$  \n",
    "We fit $f_2(x)^k\\sim (y^k-\\sigma_k(F_1^1,\\dots,F_1^K))$ then $F_2(x)^k = F_1(x)^k + \\eta f_2(x)^k \\quad\\forall k\\in\\{1,\\dots, K\\}$  \n",
    "We fit $f_3(x)^k\\sim (y^k-\\sigma_k(F_2^1,\\dots,F_2^K))$ then $F_3(x)^k = F_2(x)^k + \\eta f_3(x)^k \\quad\\forall k\\in\\{1,\\dots, K\\}$  \n",
    "$\\vdots$  \n",
    "We fit $f_m(x)^k\\sim (y^k-\\sigma_k(F_{m-1}^1,\\dots,F_{m-1}^K))$ then $F_m(x)^k = F_{m-1}(x)^k + \\eta f_m(x)^k\\quad\\forall k\\in\\{1,\\dots, K\\}$\n",
    "\n",
    "Then predictions $\\hat{p}^k = \\sigma_k(F_{m}^1,\\dots,F_{m}^K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\sigma$ is the softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the gradient boosted decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logger = logging.getLogger('GBDT')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TreeNode():\n",
    "\n",
    "    count = itertools.count()\n",
    "\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 max_depth,\n",
    "                 min_samples_split,\n",
    "                 min_samples_leaf,\n",
    "                 n_classes=2,\n",
    "                 max_features=None,\n",
    "                 depth=0,\n",
    "                 impurity='gini',\n",
    "                 is_classifier=True):\n",
    "        \"\"\"\n",
    "        A single node in a decision tree\n",
    "\n",
    "        After recursive splitting of the input data, a given node \n",
    "        represents one split of the tree if it is not a leaf node. The\n",
    "        leaf node stores the training samples in that leaf to be used \n",
    "        for prediction. \n",
    "        The splitting nodes record the feature to split on as attribute \n",
    "        self.best_feature_index and the splitting value as attribute\n",
    "        self.best_feature_split_val\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features + 1 target)\n",
    "            Note the last column of the data are the target values\n",
    "        max_depth: int\n",
    "            The maximum depth allowed when \"growing\" a tree\n",
    "        min_samples_split: int\n",
    "            The minimum number of samples required to allow a split at\n",
    "            a the node\n",
    "        min_samples_leaf: int\n",
    "            The minimum number of samples allowed in a leaf. A split\n",
    "            candidate leading to less samples in a node than the\n",
    "            min_samples_leaf will be rejected\n",
    "        n_classes: int, optional, default 2\n",
    "            Number of classes in a classification setting. Ignored when\n",
    "            self.is_classifier = False\n",
    "        max_features: int, optional, default None\n",
    "            If set to 'sqrt' then only a random subset of features are\n",
    "            used to split at the node, the number of features used in\n",
    "            this case is sqrt(n_features).\n",
    "            Else all the features are considered when splitting at this\n",
    "            node\n",
    "        depth: int, optional, default 0\n",
    "            The depth of the node in the tree\n",
    "        impurity: str, optional, default 'gini'\n",
    "            The impurity measure to use when splitting at the node.\n",
    "            I have currently only implemented two\n",
    "            'gini' - Uses the gini impurity (for classification)\n",
    "            'mse' - Uses the mean square error - equal to variance (for\n",
    "            regression)\n",
    "        is_classifier: bool, optional, default True\n",
    "            Is the tree node used as part of a classification problem\n",
    "            or a regression problem. Should be set to True if\n",
    "            classification, False if regression\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = n_classes\n",
    "        self.max_features = max_features\n",
    "        self.depth = depth\n",
    "        self.impurity = impurity\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "        self.data_shape = data.shape\n",
    "        self.split_attempted = False\n",
    "        self.best_split_impurity = None\n",
    "        self.best_feature_index = None\n",
    "        self.best_feature_split_val = None\n",
    "        self.is_leaf = False\n",
    "        self.node_impurity = self.calculate_impurity([data[:, -1]])\n",
    "        self.value = self._init_value(data)\n",
    "        self.id = str(next(self.count))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'<TreeNode '\n",
    "            f'depth:{self.depth} '\n",
    "            f'node_impurity:{self.node_impurity:.2f} '\n",
    "            f'samples:{self.data_shape[0]} '\n",
    "            f'{\"🌳\" if self.is_root else \"\"}'\n",
    "            f'{\"🍁\" if self.is_leaf else \"\"}'\n",
    "            f'>')\n",
    "\n",
    "    @property\n",
    "    def is_root(self):\n",
    "        return self.depth == 0\n",
    "\n",
    "    def info(self):\n",
    "        return dict(\n",
    "            data_shape=self.data_shape,\n",
    "            n_classes=self.n_classes,\n",
    "            depth=self.depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            node_impurity=self.node_impurity,\n",
    "            split_attempted=self.split_attempted,\n",
    "            best_split_impurity=self.best_split_impurity,\n",
    "            best_feature_index=self.best_feature_index,\n",
    "            best_feature_split_val=self.best_feature_split_val,\n",
    "            is_root=self.is_root)\n",
    "\n",
    "    def _init_value(self, data):\n",
    "        \"\"\"  \n",
    "        Returns the terminal node value based on the input data\n",
    "\n",
    "        For a classifier this is the class_counts.\n",
    "        For a regressor this is the average y value. \n",
    "\n",
    "        Note this value can be access at a splitting node to see what\n",
    "        the prediction would have been at that level of the tree\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features + 1 target)\n",
    "            Note the last column of the data are the target values\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray or float:\n",
    "            Class counts if classifier, else mean of target values \n",
    "        \"\"\"\n",
    "        if self.is_classifier:\n",
    "            return np.bincount(\n",
    "                data[:, -1].astype(int),\n",
    "                minlength=self.n_classes)\n",
    "        else:\n",
    "            return np.mean(data[:, -1])\n",
    "\n",
    "    def split(self, feature_index, feature_split_val, only_y=True):\n",
    "        \"\"\"  \n",
    "        Splits self.data on feature with index feature_index using\n",
    "        feature_split_val.\n",
    "\n",
    "        Each sample is included in left output if the feature value for\n",
    "        the sample is less than or equal to the feature_split_val else \n",
    "        it is included in the right output\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_index: int\n",
    "            Index of the feature (column) in self.data\n",
    "        feature_split_val: float\n",
    "            Feature value to use when splitting data\n",
    "        only_y: bool, optional, default True\n",
    "            Return only the y values in left and right - this is used \n",
    "            when checking candidate split purity increase\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        (numpy.ndarray, numpy.ndarray):\n",
    "            left and right splits of self.data\n",
    "        \"\"\"\n",
    "        assert feature_index in range(self.data.shape[1])\n",
    "        if only_y:\n",
    "            select = -1\n",
    "        else:\n",
    "            select = slice(None)\n",
    "        left_mask = self.data[:, feature_index] <= feature_split_val\n",
    "        right_mask = ~ left_mask\n",
    "        left = self.data[left_mask, select]\n",
    "        right = self.data[right_mask, select]\n",
    "        logger.debug(\n",
    "            f'Splitting on feature_index {feature_index} with '\n",
    "            f'feature_split_val = {feature_split_val} creates left '\n",
    "            f'with shape {left.shape} and right with '\n",
    "            f'shape {right.shape}')\n",
    "        return left, right\n",
    "\n",
    "    def gini_impurity(self, groups):\n",
    "        \"\"\"  \n",
    "        Calculate the Gini impurity for groups of values\n",
    "\n",
    "        The impurity returned is the weighted average of the impurity\n",
    "        of the groups.\n",
    "\n",
    "        You can think of gini impurity as the probability of incorrectly\n",
    "        predicting a random sample from a group if the prediction was\n",
    "        made based purely on the distribution of class labels in the\n",
    "        group\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        groups: tuple\n",
    "            The groups tuple is made up of arrays of values. It is \n",
    "            often called with groups = (left, right) to find the purity\n",
    "            of the candidate split\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Gini impurity\n",
    "        \"\"\"\n",
    "        gini = 0\n",
    "        total_samples = sum(group.shape[0] for group in groups)\n",
    "        for i, group in enumerate(groups):\n",
    "            group = group.astype(int)\n",
    "            class_counts = np.bincount(group, minlength=self.n_classes)\n",
    "            group_size = class_counts.sum()\n",
    "            class_probs = class_counts / group_size\n",
    "            unique_classes = np.count_nonzero(class_counts)\n",
    "            group_gini = (class_probs * (1 - class_probs)).sum()\n",
    "            gini += group_gini * (group_size / total_samples)\n",
    "            logger.debug(\n",
    "                f'Group {i} has size {group.shape[0]} with '\n",
    "                f'{unique_classes} unique classes '\n",
    "                f'with Gini index {group_gini:.3}')\n",
    "        return gini\n",
    "\n",
    "    def mean_square_impurity(self, groups):\n",
    "        \"\"\"  \n",
    "        Calculates the mean square error impurity\n",
    "\n",
    "        The mse impurity is the weighted average of the group variances\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        groups: tuple\n",
    "            The groups tuple is made up of arrays of values. It is \n",
    "            often called with groups = (left, right) to find the purity\n",
    "            of the candidate split\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Mean square error impurity\n",
    "        \"\"\"\n",
    "        mean_square_error = 0\n",
    "        total_samples = sum(group.shape[0] for group in groups)\n",
    "        for i, group in enumerate(groups):\n",
    "            group_size = group.shape[0]\n",
    "            group_mean = np.mean(group)\n",
    "            group_mean_square_error = np.mean((group - group_mean) ** 2)\n",
    "            mean_square_error += group_mean_square_error * \\\n",
    "                (group_size / total_samples)\n",
    "            logger.debug(\n",
    "                f'Group {i} has size {group.shape[0]} with '\n",
    "                f'with MSE impurity {group_mean_square_error:.3}')\n",
    "        logger.debug(f'MSE candidate {mean_square_error}')\n",
    "        return mean_square_error\n",
    "\n",
    "    def calculate_impurity(self, groups):\n",
    "        \"\"\"  \n",
    "        Calculates impurity based on self.impurity setting\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        groups: tuple\n",
    "            The groups tuple is made up of arrays of values. It is \n",
    "            often called with groups = (left, right) to find the purity\n",
    "            of the candidate split\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Mean square error of groups if self.impurity = 'mse'\n",
    "            Gini impurity of groups if self.impurity = 'mse'\n",
    "        \"\"\"\n",
    "        if self.impurity == 'gini':\n",
    "            return self.gini_impurity(groups)\n",
    "        elif self.impurity == 'mse':\n",
    "            return self.mean_square_impurity(groups)\n",
    "\n",
    "    def check_split(self, feature_index, feature_split_val):\n",
    "        \"\"\"  \n",
    "        Updates best split if candidate split is better\n",
    "\n",
    "        Splits the data in groups using self.split. Checks min samples\n",
    "        leaf condition after split. Calculates impurity of the split\n",
    "        then if impurity is less than best split already found and less\n",
    "        than the current node impurity the best_feature_index, the \n",
    "        best_feature_split_val and the best_split_impurity values are\n",
    "        updated.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_index: int\n",
    "            Index of the feature (column) in self.data\n",
    "        feature_split_val: float\n",
    "            Feature value to use when splitting data\n",
    "        \"\"\"\n",
    "        groups = self.split(feature_index, feature_split_val)\n",
    "        if any(len(group) < self.min_samples_leaf for group in groups):\n",
    "            logger.debug(\n",
    "                f\"Can't split node on feature {feature_index} with split \"\n",
    "                f\"val {feature_split_val} due to min_samples_leaf condition\")\n",
    "            return None\n",
    "        split_impurity = self.calculate_impurity(groups)\n",
    "        best_current_impurity = (\n",
    "            10**10 if self.best_split_impurity is None\n",
    "            else self.best_split_impurity)\n",
    "        if ((split_impurity < best_current_impurity) and\n",
    "                (split_impurity < self.node_impurity)):\n",
    "            logger.debug(\n",
    "                f'Found new best split with feature_split_val='\n",
    "                f'{feature_split_val} for feature_index = {feature_index} '\n",
    "                f'and split_impurity = {split_impurity:.2f}')\n",
    "            self.best_feature_index = feature_index\n",
    "            self.best_feature_split_val = feature_split_val\n",
    "            self.best_split_impurity = split_impurity\n",
    "\n",
    "    def find_best_split(self):\n",
    "        \"\"\"\n",
    "        Finds best split at the node\n",
    "\n",
    "        Loops through each feature and each unique value of that feature\n",
    "        checking for the best candidate split (i.e. the split that \n",
    "        reduces the impurity the most)\n",
    "\n",
    "        The function first checks if we have reached the max depth or if\n",
    "        self.data < self.min_samples_split. In either case no further\n",
    "        split is allowed and the function returns\n",
    "\n",
    "        All features are considered unless self.max_features == 'sqrt'\n",
    "        in which case a random subset of features are used of size\n",
    "        sqrt(n_features)\n",
    "        \"\"\"\n",
    "        if self.depth == self.max_depth:\n",
    "            return\n",
    "        if self.data.shape[0] < self.min_samples_split:\n",
    "            logger.info(f\"{self} can't split as samples < min_samples_split\")\n",
    "            return None\n",
    "        if self.node_impurity == 0:\n",
    "            logger.info(f\"Can't improve as node pure\")\n",
    "            return None\n",
    "        n_features = self.data.shape[1] - 1\n",
    "        all_feature_indices = np.arange(n_features)\n",
    "        if self.max_features == 'sqrt':\n",
    "            features_to_check = np.random.choice(\n",
    "                all_feature_indices,\n",
    "                size=np.sqrt(n_features).astype(int))\n",
    "        else:\n",
    "            features_to_check = all_feature_indices\n",
    "        logger.info(f'Checking features {features_to_check}')\n",
    "        for feature_index in features_to_check:\n",
    "            for feature_split_val in np.unique(self.data[:, feature_index]):\n",
    "                self.check_split(feature_index, feature_split_val)\n",
    "        self.split_attempted = True\n",
    "\n",
    "    def recursive_split(self):\n",
    "        \"\"\"  \n",
    "        Recursively grows tree by splitting to reduce impurity the most\n",
    "\n",
    "        The function finds the best split using the find_best_split\n",
    "        method. If there was a split found two nodes are created - left\n",
    "        and right. Finally the recursive_split method is called on each\n",
    "        of the new nodes.\n",
    "\n",
    "        Note the depth of the children node is incremented, otherwise\n",
    "        the node settings such as min_samples_split are passed to the\n",
    "        children nodes\n",
    "        \"\"\"\n",
    "        self.find_best_split()\n",
    "        if self.best_feature_index is not None:\n",
    "            logger.info(f'Splitting tree on feature_index '\n",
    "                        f'{self.best_feature_index} and feature_split_val '\n",
    "                        f'{self.best_feature_split_val:.2f}')\n",
    "            left, right = self.split(\n",
    "                feature_index=self.best_feature_index,\n",
    "                feature_split_val=self.best_feature_split_val,\n",
    "                only_y=False)\n",
    "            del self.data\n",
    "            self.left = TreeNode(\n",
    "                data=left,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                n_classes=self.n_classes,\n",
    "                max_features=self.max_features,\n",
    "                depth=self.depth + 1,\n",
    "                impurity=self.impurity,\n",
    "                is_classifier=self.is_classifier)\n",
    "            self.right = TreeNode(\n",
    "                data=right,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                n_classes=self.n_classes,\n",
    "                max_features=self.max_features,\n",
    "                depth=self.depth + 1,\n",
    "                impurity=self.impurity,\n",
    "                is_classifier=self.is_classifier)\n",
    "            self.left.recursive_split()\n",
    "            self.right.recursive_split()\n",
    "        else:\n",
    "            logger.info('Reached max depth or no splits reduce impurity')\n",
    "            self.is_leaf = True\n",
    "\n",
    "    def walk_depth_first(self, only_leaves=True):\n",
    "        \"\"\"  \n",
    "        Generator traversing of all nodes below and including this node\n",
    "\n",
    "        Depth first so visiting children before siblings\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        only_leaves: bool, optional, default True\n",
    "            Only return leaf nodes\n",
    "\n",
    "        Yields:\n",
    "            TreeNode: each node in tree\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            yield self\n",
    "        else:\n",
    "            if not only_leaves:\n",
    "                yield self\n",
    "            for node in (self.left, self.right):\n",
    "                yield from node.walk_depth_first(only_leaves)\n",
    "\n",
    "    def walk_breadth_first(self, layer=None):\n",
    "        \"\"\"  \n",
    "        Generator traversing of all nodes below and including this node\n",
    "\n",
    "        Breadth first so visiting siblings before children\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        only_leaves: bool, optional, default True\n",
    "            Only return leaf nodes\n",
    "\n",
    "        Yields:\n",
    "            TreeNode: each node in tree\n",
    "        \"\"\"\n",
    "        if layer is None:\n",
    "            layer = [self]\n",
    "        for node in layer:\n",
    "            yield node\n",
    "        new_layer = [\n",
    "            child\n",
    "            for node_children in [[node.left, node.right]\n",
    "                                  for node in layer if not node.is_leaf]\n",
    "            for child in node_children]\n",
    "        if new_layer:\n",
    "            yield from self.walk_breadth_first(new_layer)\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"  \n",
    "        prints ascii representation of tree below this node\n",
    "        \"\"\"\n",
    "        for node in self.walk_depth_first(only_leaves=False):\n",
    "            print('--' * node.depth + str(node))\n",
    "\n",
    "    def predict_row_proba(self, row):\n",
    "        \"\"\"\n",
    "        Predicts class probabilities for input row by walking the tree\n",
    "        and returning the leaf node class probabilities\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        row: numpy.ndarray\n",
    "            Input row, shape (n features,)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Class probabilities, shape (n classes, )\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            group_size = self.value.sum()\n",
    "            class_probs = self.value / group_size\n",
    "            return class_probs\n",
    "        elif row[self.best_feature_index] <= self.best_feature_split_val:\n",
    "            return self.left.predict_row_proba(row)\n",
    "        else:\n",
    "            return self.right.predict_row_proba(row)\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        \"\"\"Predicts class probabilities for input data\n",
    "\n",
    "        Predicts class probabilities for each row in data by walking the\n",
    "        tree and returning the leaf node class probabilities\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted sample class probabilities, \n",
    "            shape (m samples, n classes)\n",
    "        \"\"\"\n",
    "        if not self.is_classifier:\n",
    "            raise Exception('Not a classifier')\n",
    "        if len(data.shape) == 2:\n",
    "            return np.stack([self.predict_row_proba(row)\n",
    "                             for row in data])\n",
    "        else:\n",
    "            return self.predict_row_proba(data)\n",
    "\n",
    "    def predict_regressor_row(self, row):\n",
    "        \"\"\"\n",
    "        Predicts target value for input row by walking the tree\n",
    "        and returning the leaf node value\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        row: numpy.ndarray\n",
    "            Input row, shape (n features,)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float:\n",
    "            Predicted target value\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            return self.value\n",
    "        elif row[self.best_feature_index] <= self.best_feature_split_val:\n",
    "            return self.left.predict_regressor_row(row)\n",
    "        else:\n",
    "            return self.right.predict_regressor_row(row)\n",
    "\n",
    "    def predict_regressor(self, data):\n",
    "        \"\"\"  \n",
    "        Predicts target values for each row in data by walking the\n",
    "        tree and returning the leaf node values\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values, shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        if len(data.shape) == 2:\n",
    "            return np.stack([self.predict_regressor_row(row)\n",
    "                             for row in data])\n",
    "        else:\n",
    "            return self.predict_regressor_row(data)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predicts target values or class labels for classification\n",
    "\n",
    "        Predicts target values/class for each row in data by walking the\n",
    "        tree and returning the leaf node value for regression or the \n",
    "        class with the largest predicted probability for classification\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values or class labels for classification\n",
    "        \"\"\"\n",
    "        if self.is_classifier:\n",
    "            return np.argmax(self.predict_proba(data), axis=-1)\n",
    "        else:\n",
    "            return self.predict_regressor(data)\n",
    "\n",
    "    def dot(self,\n",
    "            feature_names,\n",
    "            samples=True,\n",
    "            impurity=True,\n",
    "            value=True):\n",
    "        \"\"\"  \n",
    "        Returns Digraph visualizing the tree below this node\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_names: list[str]\n",
    "            List of feature names\n",
    "        samples: bool, optional, default True\n",
    "            Whether to display the number of samples on this node\n",
    "        impurity: bool, optional, default True\n",
    "            Whether to display the impurity value on this node\n",
    "        value: bool, optional, default True\n",
    "            Whether to dispaly the value on this node\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        graphviz.Digraph:\n",
    "            dot for tree diagram visual\n",
    "        \"\"\"\n",
    "        dot = Digraph(\n",
    "            comment='Decsion Tree',\n",
    "            node_attr=dict(shape=\"rectangle\",\n",
    "                           style=\"rounded\",\n",
    "                           fillcolor=\"#028d35\"))\n",
    "        for i, node in enumerate(self.walk_breadth_first()):\n",
    "            label = \"\"\n",
    "            if not node.is_leaf:\n",
    "                label += (\n",
    "                    f'{feature_names[node.best_feature_index]} <= '\n",
    "                    f'{node.best_feature_split_val}\\n')\n",
    "                dot.edge(node.id, node.left.id)\n",
    "                dot.edge(node.id, node.right.id)\n",
    "            if samples:\n",
    "                label += f'Samples = {node.data_shape[0]}\\n'\n",
    "            if impurity:\n",
    "                label += f'Impurity = {node.node_impurity:.2f}\\n'\n",
    "            if value:\n",
    "                if self.is_classifier:\n",
    "                    label += f'Class counts = {str(node.value)}\\n'\n",
    "                else:\n",
    "                    label += f'Average y = {node.value:.2f}\\n'\n",
    "            dot.node(name=node.id, label=label)\n",
    "        return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_depth=2,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 n_classes=2,\n",
    "                 max_features=None,\n",
    "                 impurity='gini',\n",
    "                 is_classifier=True):\n",
    "        \"\"\"Decision tree model\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        max_depth: int\n",
    "            The maximum depth allowed when \"growing\" a tree\n",
    "        min_samples_split: int\n",
    "            The minimum number of samples required to allow a split at a\n",
    "            node\n",
    "        min_samples_leaf: int\n",
    "            The minimum number of samples allowed in a leaf. A split\n",
    "            candidate leading to less samples in a node than the\n",
    "            min_samples_leaf will be rejected\n",
    "        n_classes: int, optional, default 2\n",
    "            Number of classes in a classification setting. Ignored when\n",
    "            self.is_classifier = False\n",
    "        max_features: int, optional, default None\n",
    "            If set to 'sqrt' then only a random subset of features are\n",
    "            used to split at each node, the number of features used in\n",
    "            this case is sqrt(n_features).\n",
    "            Else all the features are considered when splitting at each\n",
    "            node\n",
    "        impurity: str, optional, default 'gini'\n",
    "            The impurity measure to use when splitting at each node.\n",
    "            I have currently only implemented two\n",
    "            'gini' - Uses the gini impurity (for classification)\n",
    "            'mse' - Uses the mean square error - equal to variance (for\n",
    "            regression)\n",
    "        is_classifier: bool, optional, default True\n",
    "            Is the model used as part of a classification problem\n",
    "            or a regression problem. Should be set to True if\n",
    "            classification, False if regression\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = n_classes\n",
    "        self.max_features = max_features\n",
    "        self.impurity = impurity\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "        self.is_fitted = False\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the decision tree model\n",
    "\n",
    "        The tree is fitted by instantiaing a root TreeNode instance and\n",
    "        then calling the recursive_split method. This iteratively grows\n",
    "        the tree by finding the best split to reduce the impurity the\n",
    "        most.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, n features)\n",
    "        y: numpy.ndarray\n",
    "            Target values, shape (m samples, 1)\n",
    "            If classifier with n_classes the values are assumed to be in\n",
    "            0, ..., n-1\n",
    "        \"\"\"\n",
    "        y_shape = (X.shape[0], 1)\n",
    "        data = np.concatenate((X, y.reshape(y_shape)), axis=1)\n",
    "        self.tree = TreeNode(\n",
    "            data=data,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            n_classes=self.n_classes,\n",
    "            max_features=self.max_features,\n",
    "            impurity=self.impurity,\n",
    "            is_classifier=self.is_classifier)\n",
    "        self.tree.recursive_split()\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predicts target values or class labels for classification\n",
    "\n",
    "        Predicts target values/class for each row in data by walking the\n",
    "        tree and returning the leaf node value for regression or the \n",
    "        class with the largest predicted probability for classification\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values or class labels for classification\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise Exception('Decision tree not fitted')\n",
    "        return self.tree.predict(data)\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        \"\"\"Predicts class probabilities for input data\n",
    "\n",
    "        Predicts class probabilities for each row in data by walking the\n",
    "        tree and returning the leaf node class probabilities\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: numpy.ndarray\n",
    "            The input data with shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted sample class probabilities, \n",
    "            shape (m samples, n classes)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise Exception('Decision tree not fitted')\n",
    "        return self.tree.predict_proba(data)\n",
    "\n",
    "    def render(self, feature_names):\n",
    "        \"\"\"Returns Digraph visualizing the decision tree (if fitted)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature_names: list[str]\n",
    "            List of feature names\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        graphviz.Digraph:\n",
    "            dot for tree diagram visual\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            print('Decision tree not fitted')\n",
    "        else:\n",
    "            return self.tree.dot(feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostedDecisionTree():\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_depth=2,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 n_classes=2,\n",
    "                 max_features=None,\n",
    "                 is_classifier=True,\n",
    "                 n_trees=10,\n",
    "                 learning_rate=0.1):\n",
    "        \"\"\"Gradient boosted decision tree model\n",
    "\n",
    "        The trees are grown sequentially and fitted to the negative \n",
    "        gradient of the cost function with respect to the raw predicted\n",
    "        values at the previous stage. \n",
    "\n",
    "        Note I use the term raw_predictions as raw predicted values \n",
    "        must be transformed to find the probability estimates in the \n",
    "        case of classification.\n",
    "\n",
    "        In practice these gradients are equal to the residual.\n",
    "\n",
    "        The raw predictions for a stage are made by adding the new delta\n",
    "        model (multiplied by the learning rate) to the raw predictions\n",
    "        from the previous stage\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        max_depth: int\n",
    "            The maximum depth allowed when \"growing\" a tree\n",
    "        min_samples_split: int\n",
    "            The minimum number of samples required to allow a split at a\n",
    "            node\n",
    "        min_samples_leaf: int\n",
    "            The minimum number of samples allowed in a leaf. A split\n",
    "            candidate leading to less samples in a node than the\n",
    "            min_samples_leaf will be rejected\n",
    "        n_classes: int, optional, default 2\n",
    "            Number of classes in a classification setting. Ignored when\n",
    "            self.is_classifier = False\n",
    "        max_features: int, optional, default None\n",
    "            If set to 'sqrt' then only a random subset of features are\n",
    "            used to split at each node, the number of features used in\n",
    "            this case is sqrt(n_features).\n",
    "            Else all the features are considered when splitting at each\n",
    "            node\n",
    "        is_classifier: bool, optional, default True\n",
    "            Is the model used as part of a classification problem\n",
    "            or a regression problem. Should be set to True if\n",
    "            classification, False if regression\n",
    "        n_trees: int, optional, default 10\n",
    "            Number of trees, equivalently gradient steps\n",
    "        learning_rate: float, optional, default 0.05\n",
    "            The learning rate parameter controlling the gradient descent\n",
    "            step size\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = n_classes\n",
    "        self.max_features = max_features\n",
    "        self.is_classifier = is_classifier\n",
    "\n",
    "        self.n_trees = n_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_fitted = False\n",
    "        np.random.seed(1)\n",
    "        self.trees_to_fit = 1 if n_classes <= 2 else n_classes\n",
    "        self.trees = [\n",
    "            [None for _ in range(self.trees_to_fit)]\n",
    "            for _ in range(self.n_trees)]\n",
    "        #  trees has shape (n_trees, n_classes)\n",
    "\n",
    "    def predict_delta_model(self, X, stage=0):\n",
    "        \"\"\"Calculate the delta model for a stage\n",
    "\n",
    "        This function returns the estimate of the negative gradient. \n",
    "        These raw predictions are the delta models f_{stage + 1}\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Sample data, shape (m samples, n features)\n",
    "        stage: int, optional, default 0\n",
    "            What correction step are we predicting\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            gradient_step, shape (X.shape[0], n_classes)\n",
    "            if n_classes > 2 else shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        class_gradient_step = []\n",
    "        for class_k, model in enumerate(self.trees[stage]):\n",
    "            k_gradient_step = model.predict(X).reshape(-1)\n",
    "            class_gradient_step.append(k_gradient_step)\n",
    "        gradient_step = np.stack(class_gradient_step, axis=-1)\n",
    "        return gradient_step\n",
    "\n",
    "    def predict_raw_stages(self, X, n_stages=None):\n",
    "        \"\"\"Predictions for input X\n",
    "\n",
    "        The predictions are given by the transformed sum of initial \n",
    "        model and delta models. Note no transformation is required for\n",
    "        regression.\n",
    "\n",
    "        If n_stages specified stop at that stage. The delta model is\n",
    "        multiplied by the learning rate before being added to the\n",
    "        raw predictions\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Sample data, shape (m samples, n features)\n",
    "        n_stages: in, optional, default None\n",
    "            If given return prediction an n_stages\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            predictions, shape (X.shape[0], n_classes)\n",
    "            if n_classes > 2 else shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        if not n_stages:\n",
    "            n_stages = self.n_trees\n",
    "        if n_stages not in list(range(1, self.n_trees + 1)):\n",
    "            raise Exception('n_stages must be between 1 and n_trees')\n",
    "        raw_predictions = self.f_0_prediction(X)\n",
    "        for stage in range(n_stages):\n",
    "            stage_gradient_step = self.predict_delta_model(X, stage)\n",
    "            raw_predictions += self.learning_rate * stage_gradient_step\n",
    "        return self.convert_raw_predictions(raw_predictions)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts target values or class labels for classification\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Sample data, shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted target values or class labels for classification\n",
    "        \"\"\"\n",
    "        if not self.is_classifier:\n",
    "            return self.predict_raw_stages(X)\n",
    "        else:\n",
    "            return np.argmax(self.predict_proba(X), axis=-1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predicts class probabilities for input data\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Sample data, shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            Predicted sample class probabilities, \n",
    "            shape (m samples, n classes)\n",
    "            if n_classes > 2 else shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        if not self.is_classifier:\n",
    "            raise Exception('Not a classifier')\n",
    "        if self.n_classes == 2:\n",
    "            prob_class_one = self.predict_raw_stages(X)\n",
    "            return np.stack([1-prob_class_one, prob_class_one], axis=-1)\n",
    "        if self.n_classes > 2:\n",
    "            return self.predict_raw_stages(X)\n",
    "\n",
    "    def convert_raw_predictions(self, raw_predictions):\n",
    "        \"\"\"Convert raw_predictions to probability if classifier\n",
    "\n",
    "        This uses sigmoid if the are two classes - in which case we\n",
    "        model the logit. Softmax function is used when there are more\n",
    "        than two classes.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        raw_predictions: numpy.ndarray\n",
    "            Raw predictions, shape (m samples, n classes)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            target values or class probabilities for classification\n",
    "        \"\"\"\n",
    "        if not self.is_classifier:\n",
    "            return raw_predictions\n",
    "        if self.is_classifier and self.n_classes == 2:\n",
    "            return expit(raw_predictions)\n",
    "        if self.is_classifier and self.n_classes > 2:\n",
    "            return np.exp(\n",
    "                raw_predictions - logsumexp(raw_predictions, axis=1)[:, None])\n",
    "\n",
    "    def f_0_prediction(self, X):\n",
    "        \"\"\"Return initial raw_predictions for X\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, n features)\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            raw_predictions, shape (m samples, n classes)\n",
    "            if n_classes > 2 else shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        if not self.is_classifier:\n",
    "            return self.regression_f_0_tree.predict(X).reshape(n, 1)\n",
    "        if self.is_classifier and self.n_classes == 2:\n",
    "            return np.repeat(self.f_0, n).reshape(n, 1)\n",
    "        if self.is_classifier and self.n_classes > 2:\n",
    "            return np.repeat(self.f_0, n, axis=0)\n",
    "\n",
    "    def init_f_0(self, X, y):\n",
    "        \"\"\"Fit initial prediction model\n",
    "\n",
    "        For regression this is simple fitting a first tree to the target\n",
    "        values.\n",
    "\n",
    "        For classification when we model the logit (in two class \n",
    "        scenario) we use the logit of the average probability in the\n",
    "        training data.\n",
    "        For the multi class case, where we model the log of each class\n",
    "        probability as an additive model, we initialise the raw values\n",
    "        as the log of the observed probability of that class.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, n features)\n",
    "        y: numpy.ndarray\n",
    "            Target values, shape (m samples, 1)\n",
    "            If classifier with n_classes the values are assumed to be in\n",
    "            0, ..., n-1\n",
    "        \"\"\"\n",
    "        y = y.reshape(-1)\n",
    "        if not self.is_classifier:\n",
    "            self.regression_f_0_tree = self.get_tree()\n",
    "            self.regression_f_0_tree.fit(X, y)\n",
    "        if self.is_classifier and self.n_classes == 2:\n",
    "            self.f_0 = np.log(y.sum() / (y.shape[0] - y.sum()))\n",
    "        if self.is_classifier and self.n_classes > 2:\n",
    "            self.f_0 = np.log(\n",
    "                np.bincount(y, minlength=self.n_classes) / y.shape[0])[None, :]\n",
    "\n",
    "    def get_tree(self):\n",
    "        \"\"\"Helper to return decision tree to be fitted\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        DecisionTree:\n",
    "            Regression tree\n",
    "        \"\"\"\n",
    "        return DecisionTree(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            n_classes=self.n_classes,\n",
    "            max_features=self.max_features,\n",
    "            impurity='mse',\n",
    "            is_classifier=False)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the gradient boosted decision tree\n",
    "\n",
    "        For each stage fit a tree to the negative gradient (for that\n",
    "        class), then update the raw predictions using the learning rate\n",
    "        and delta model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, n features)\n",
    "        y: numpy.ndarray\n",
    "            Target values, shape (m samples, 1)\n",
    "            If classifier with n_classes the values are assumed to be in\n",
    "            0, ..., n-1\n",
    "        \"\"\"\n",
    "        if self.is_classifier:\n",
    "            y = y.astype(int)\n",
    "        self.init_f_0(X, y)\n",
    "        prev_stage_raw_predictions = self.f_0_prediction(X)\n",
    "        for stage in range(self.n_trees):\n",
    "            negative_gradient = self.negative_gradient(\n",
    "                y, prev_stage_raw_predictions)\n",
    "            self.fit_stage(X, negative_gradient, stage=stage)\n",
    "            delta_model = self.predict_delta_model(X, stage=stage)\n",
    "            prev_stage_raw_predictions = prev_stage_raw_predictions + \\\n",
    "                (self.learning_rate * delta_model)\n",
    "\n",
    "    def fit_stage(self, X, negative_gradient, stage=0):\n",
    "        \"\"\"Fit a given stage\n",
    "\n",
    "        For regression this is just fitting a single tree to the\n",
    "        gradient. For classification we fit one tree for each class (\n",
    "        unless there are only two classes when we can use just one)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Training data, shape (m samples, n features)\n",
    "        negative_gradient: numpy.ndarray\n",
    "            dL_dY^hat, shape (m samples, n features)\n",
    "        stage: int, optional, default 0\n",
    "            stage to fit\n",
    "        \"\"\"\n",
    "        logger.info(f'Fitting stage {stage}')\n",
    "        trees_to_fit = 1 if self.n_classes <= 2 else self.n_classes\n",
    "        for class_k in range(trees_to_fit):\n",
    "            target = negative_gradient[:, class_k]\n",
    "            tree = self.get_tree()\n",
    "            tree.fit(X, target)\n",
    "            self.trees[stage][class_k] = tree\n",
    "\n",
    "    def negative_gradient(self, y, prev_stage_raw_predictions):\n",
    "        \"\"\"Gradient of the loss function with res\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        y: numpy.ndarray\n",
    "            Target values, shape (m samples, 1)\n",
    "            If classifier with n_classes the values are assumed to be in\n",
    "            0, ..., n-1\n",
    "        prev_stage_raw_predictions: numpy.ndarray\n",
    "            raw_predictions, shape\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        numpy.ndarray:\n",
    "            negative gradient, shape (m samples, n classes)\n",
    "            if n_classes > 2 else shape (m samples, 1)\n",
    "        \"\"\"\n",
    "        if self.is_classifier and self.n_classes > 2:\n",
    "            y = np.eye(self.n_classes)[y.reshape(-1)]\n",
    "        else:\n",
    "            y = y.reshape(y.shape[0], 1)\n",
    "        return y - self.convert_raw_predictions(prev_stage_raw_predictions)\n",
    "\n",
    "    def render(self, stage, class_k, feature_names):\n",
    "        \"\"\"Returns Digraph visualizing one of the decision trees\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        stage: [type]\n",
    "            Stage to get tree from\n",
    "        class_k: [type]\n",
    "            tree for class class_k\n",
    "        feature_names: [type]\n",
    "            Feature names\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        graphviz.Digraph:\n",
    "            dot for tree diagram visual\n",
    "        \"\"\"\n",
    "        return self.trees[stage][class_k].render(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Small dummy data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gbdt = GradientBoostedDecisionTree(max_depth=2, n_trees=4, n_classes=1, is_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val -2.06\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val -2.30\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 1.13\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Fitting stage 0\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 0.58\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val -0.88\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 1.13\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Fitting stage 1\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 0.32\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val -0.76\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 1.13\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Fitting stage 2\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 0.58\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val -0.88\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 1.13\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Fitting stage 3\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 0.58\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val -0.76\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 1.13\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error at stage 1 is 25.21\n",
      "Total error at stage 2 is 23.54\n",
      "Total error at stage 3 is 22.17\n",
      "Total error at stage 4 is 21.02\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(size=(20,1))\n",
    "y = (4 * x*x + x + np.random.normal(scale=.1, size=(20,1))).reshape(20,1)\n",
    "X=x\n",
    "\n",
    "gbdt.fit(X,y)\n",
    "\n",
    "# ok check tree\n",
    "# tree  = gbdt.get_tree()\n",
    "# tree.fit(X,y)\n",
    "# tree.render(['x'])\n",
    "\n",
    "gbdt.render(2,0,'x')\n",
    "\n",
    "for stage in [1,2,3,4]:\n",
    "    # print(y - gbdt.predict_raw_stages(X, stage))\n",
    "    total_stage_error = abs(y - gbdt.predict_raw_stages(X, stage)).sum()\n",
    "    print(f'Total error at stage {stage} is {total_stage_error:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Gradient boosted decision tree classifier - Iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load the iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  y\n",
       "0                6.1               2.8                4.7               1.2  1\n",
       "1                5.7               3.8                1.7               0.3  0\n",
       "2                7.7               2.6                6.9               2.3  2\n",
       "3                6.0               2.9                4.5               1.5  1\n",
       "4                6.8               2.8                4.8               1.4  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data = load_iris()\n",
    "iris_df = pd.DataFrame(iris_data['data'],columns=iris_data['feature_names'])\n",
    "iris_df['y'] = iris_data['target']\n",
    "iris_df = iris_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "iris_sample = iris_df.head(5)\n",
    "iris_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit gradient boosted decision tree classifier and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GBDT:Fitting stage 0\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Can't improve as node pure\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 5.00\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 4.30\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.70\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.70\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.90\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.80\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Fitting stage 1\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 4.30\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.90\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Can't improve as node pure\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.70\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.60\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.90\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.70\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Fitting stage 2\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Can't improve as node pure\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.90\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 5.30\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.70\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.60\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.90\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.70\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Fitting stage 3\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 5.30\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.90\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 1.90\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 0 and feature_split_val 4.60\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.60\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.60\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 2 and feature_split_val 4.90\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Checking features [0 1 2 3]\n",
      "INFO:GBDT:Splitting tree on feature_index 3 and feature_split_val 1.70\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n",
      "INFO:GBDT:Reached max depth or no splits reduce impurity\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"540pt\" height=\"258pt\"\n",
       " viewBox=\"0.00 0.00 539.61 258.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 254)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-254 535.61,-254 535.61,4 -4,4\"/>\n",
       "<!-- 92 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>92</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M325.06,-250C325.06,-250 197.22,-250 197.22,-250 191.22,-250 185.22,-244 185.22,-238 185.22,-238 185.22,-198 185.22,-198 185.22,-192 191.22,-186 197.22,-186 197.22,-186 325.06,-186 325.06,-186 331.06,-186 337.06,-192 337.06,-198 337.06,-198 337.06,-238 337.06,-238 337.06,-244 331.06,-250 325.06,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.14\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal length (cm) &lt;= 1.9</text>\n",
       "<text text-anchor=\"middle\" x=\"261.14\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 150</text>\n",
       "<text text-anchor=\"middle\" x=\"261.14\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.18</text>\n",
       "<text text-anchor=\"middle\" x=\"261.14\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = &#45;0.00</text>\n",
       "</g>\n",
       "<!-- 93 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>93</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M241.12,-150C241.12,-150 111.16,-150 111.16,-150 105.16,-150 99.16,-144 99.16,-138 99.16,-138 99.16,-98 99.16,-98 99.16,-92 105.16,-86 111.16,-86 111.16,-86 241.12,-86 241.12,-86 247.12,-86 253.12,-92 253.12,-98 253.12,-98 253.12,-138 253.12,-138 253.12,-144 247.12,-150 241.12,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"176.14\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">sepal length (cm) &lt;= 5.3</text>\n",
       "<text text-anchor=\"middle\" x=\"176.14\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 50</text>\n",
       "<text text-anchor=\"middle\" x=\"176.14\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.00</text>\n",
       "<text text-anchor=\"middle\" x=\"176.14\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 0.60</text>\n",
       "</g>\n",
       "<!-- 92&#45;&gt;93 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>92&#45;&gt;93</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M234.24,-185.99C226.49,-177.06 217.94,-167.19 209.83,-157.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"212.35,-155.4 203.15,-150.14 207.06,-159.99 212.35,-155.4\"/>\n",
       "</g>\n",
       "<!-- 94 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>94</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M411.06,-150C411.06,-150 283.22,-150 283.22,-150 277.22,-150 271.22,-144 271.22,-138 271.22,-138 271.22,-98 271.22,-98 271.22,-92 277.22,-86 283.22,-86 283.22,-86 411.06,-86 411.06,-86 417.06,-86 423.06,-92 423.06,-98 423.06,-98 423.06,-138 423.06,-138 423.06,-144 417.06,-150 411.06,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.14\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal length (cm) &lt;= 4.9</text>\n",
       "<text text-anchor=\"middle\" x=\"347.14\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"347.14\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.00</text>\n",
       "<text text-anchor=\"middle\" x=\"347.14\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = &#45;0.30</text>\n",
       "</g>\n",
       "<!-- 92&#45;&gt;94 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>92&#45;&gt;94</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M288.35,-185.99C296.19,-177.06 304.85,-167.19 313.05,-157.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"315.84,-159.97 319.81,-150.14 310.58,-155.35 315.84,-159.97\"/>\n",
       "</g>\n",
       "<!-- 95 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>95</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.42,-50C100.42,-50 11.86,-50 11.86,-50 5.86,-50 -0.14,-44 -0.14,-38 -0.14,-38 -0.14,-12 -0.14,-12 -0.14,-6 5.86,0 11.86,0 11.86,0 100.42,0 100.42,0 106.42,0 112.42,-6 112.42,-12 112.42,-12 112.42,-38 112.42,-38 112.42,-44 106.42,-50 100.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 40</text>\n",
       "<text text-anchor=\"middle\" x=\"56.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.00</text>\n",
       "<text text-anchor=\"middle\" x=\"56.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 0.60</text>\n",
       "</g>\n",
       "<!-- 93&#45;&gt;95 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>93&#45;&gt;95</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.18,-85.94C122.48,-76.31 108.51,-65.72 95.89,-56.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.91,-53.28 87.83,-50.03 93.68,-58.86 97.91,-53.28\"/>\n",
       "</g>\n",
       "<!-- 96 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>96</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.42,-50C230.42,-50 141.86,-50 141.86,-50 135.86,-50 129.86,-44 129.86,-38 129.86,-38 129.86,-12 129.86,-12 129.86,-6 135.86,0 141.86,0 141.86,0 230.42,0 230.42,0 236.42,0 242.42,-6 242.42,-12 242.42,-12 242.42,-38 242.42,-38 242.42,-44 236.42,-50 230.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 10</text>\n",
       "<text text-anchor=\"middle\" x=\"186.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.00</text>\n",
       "<text text-anchor=\"middle\" x=\"186.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 0.60</text>\n",
       "</g>\n",
       "<!-- 93&#45;&gt;96 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>93&#45;&gt;96</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M179.55,-85.94C180.47,-77.59 181.47,-68.52 182.4,-60.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.88,-60.35 183.5,-50.03 178.93,-59.59 185.88,-60.35\"/>\n",
       "</g>\n",
       "<!-- 97 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>97</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M384.58,-50C384.58,-50 291.7,-50 291.7,-50 285.7,-50 279.7,-44 279.7,-38 279.7,-38 279.7,-12 279.7,-12 279.7,-6 285.7,0 291.7,0 291.7,0 384.58,0 384.58,0 390.58,0 396.58,-6 396.58,-12 396.58,-12 396.58,-38 396.58,-38 396.58,-44 390.58,-50 384.58,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"338.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 54</text>\n",
       "<text text-anchor=\"middle\" x=\"338.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.00</text>\n",
       "<text text-anchor=\"middle\" x=\"338.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = &#45;0.30</text>\n",
       "</g>\n",
       "<!-- 94&#45;&gt;97 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>94&#45;&gt;97</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M344.07,-85.94C343.24,-77.59 342.34,-68.52 341.5,-60.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"344.98,-59.64 340.52,-50.03 338.02,-60.33 344.98,-59.64\"/>\n",
       "</g>\n",
       "<!-- 98 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>98</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M519.58,-50C519.58,-50 426.7,-50 426.7,-50 420.7,-50 414.7,-44 414.7,-38 414.7,-38 414.7,-12 414.7,-12 414.7,-6 420.7,0 426.7,0 426.7,0 519.58,0 519.58,0 525.58,0 531.58,-6 531.58,-12 531.58,-12 531.58,-38 531.58,-38 531.58,-44 525.58,-50 519.58,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"473.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 46</text>\n",
       "<text text-anchor=\"middle\" x=\"473.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 0.00</text>\n",
       "<text text-anchor=\"middle\" x=\"473.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = &#45;0.30</text>\n",
       "</g>\n",
       "<!-- 94&#45;&gt;98 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>94&#45;&gt;98</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M390.15,-85.94C403.48,-76.31 418.15,-65.72 431.39,-56.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"433.81,-58.72 439.87,-50.03 429.71,-53.05 433.81,-58.72\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x12d3a5e90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_sample_vals = iris_sample.values\n",
    "\n",
    "# # for small sample\n",
    "# X = iris_sample_vals[:,:-1]\n",
    "# y = iris_sample_vals[:,-1]\n",
    "\n",
    "X = iris_df.values[:,:-1]\n",
    "y = iris_df.values[:,-1]\n",
    "\n",
    "gbdt = GradientBoostedDecisionTree(n_classes=3, n_trees=4)\n",
    "gbdt.fit(X, y)\n",
    "\n",
    "feature_names = iris_data['feature_names']\n",
    "gbdt.render(stage=3, class_k=0, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Example prediction on Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29276876, 0.41410537, 0.29312588],\n",
       "       [0.42221886, 0.28880079, 0.28898035],\n",
       "       [0.28885716, 0.29160327, 0.41953957],\n",
       "       [0.29276876, 0.41410537, 0.29312588]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt.predict_proba(X[0:4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0,\n",
       "       0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1,\n",
       "       1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 1,\n",
       "       0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 1, 0, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0,\n",
       "       0, 1, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1,\n",
       "       1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2,\n",
       "       0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Gradient boosted decision tree classifier - Titanic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load titanic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_feather('../data/titanic/processed/X_train.feather')\n",
    "y_train = pd.read_feather('../data/titanic/processed/y_train.feather')\n",
    "X_test = pd.read_feather('../data/titanic/processed/X_test.feather')\n",
    "y_test = pd.read_feather('../data/titanic/processed/y_test.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gradient boosted decision tree model accuracy on Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 83.24%\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.WARNING)\n",
    "titanic_gbdt = GradientBoostedDecisionTree(max_depth=8, n_trees=10, learning_rate=0.5)\n",
    "titanic_gbdt.fit(X_train.values, y_train.values)\n",
    "y_pred = titanic_gbdt.predict(X_test.values)\n",
    "test_acc = (y_pred.flatten() == y_test.values.flatten()).sum() / len(y_test)\n",
    "print(f'Test accuracy = {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Gradient boosted decision tree regressor - Boston housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load Boston housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510</td>\n",
       "      <td>6.416</td>\n",
       "      <td>84.1</td>\n",
       "      <td>2.6463</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>395.50</td>\n",
       "      <td>9.04</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05644</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447</td>\n",
       "      <td>6.758</td>\n",
       "      <td>32.9</td>\n",
       "      <td>4.0776</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>3.53</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.09164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>6.065</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>390.91</td>\n",
       "      <td>5.52</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.09017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>6.297</td>\n",
       "      <td>91.8</td>\n",
       "      <td>2.3682</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>385.09</td>\n",
       "      <td>17.27</td>\n",
       "      <td>16.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "0  0.09178   0.0   4.05   0.0  0.510  6.416  84.1  2.6463   5.0  296.0   \n",
       "1  0.05644  40.0   6.41   1.0  0.447  6.758  32.9  4.0776   4.0  254.0   \n",
       "2  0.10574   0.0  27.74   0.0  0.609  5.983  98.8  1.8681   4.0  711.0   \n",
       "3  0.09164   0.0  10.81   0.0  0.413  6.065   7.8  5.2873   4.0  305.0   \n",
       "4  5.09017   0.0  18.10   0.0  0.713  6.297  91.8  2.3682  24.0  666.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT     y  \n",
       "0     16.6  395.50   9.04  23.6  \n",
       "1     17.6  396.90   3.53  32.4  \n",
       "2     20.1  390.11  18.07  13.6  \n",
       "3     19.2  390.91   5.52  22.8  \n",
       "4     20.2  385.09  17.27  16.1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data = load_boston()\n",
    "boston_df = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names'])\n",
    "boston_df['y'] = boston_data['target']\n",
    "boston_df = boston_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "boston_sample = boston_df.head(5)\n",
    "boston_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit gradient boosted decision tree on Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"517pt\" height=\"258pt\"\n",
       " viewBox=\"0.00 0.00 516.61 258.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 254)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-254 512.61,-254 512.61,4 -4,4\"/>\n",
       "<!-- 3895 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>3895</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M297.58,-250C297.58,-250 204.7,-250 204.7,-250 198.7,-250 192.7,-244 192.7,-238 192.7,-238 192.7,-198 192.7,-198 192.7,-192 198.7,-186 204.7,-186 204.7,-186 297.58,-186 297.58,-186 303.58,-186 309.58,-192 309.58,-198 309.58,-198 309.58,-238 309.58,-238 309.58,-244 303.58,-250 297.58,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.14\" y=\"-234.8\" font-family=\"Times,serif\" font-size=\"14.00\">TAX &lt;= 222.0</text>\n",
       "<text text-anchor=\"middle\" x=\"251.14\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 404</text>\n",
       "<text text-anchor=\"middle\" x=\"251.14\" y=\"-206.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 17.51</text>\n",
       "<text text-anchor=\"middle\" x=\"251.14\" y=\"-192.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = &#45;0.00</text>\n",
       "</g>\n",
       "<!-- 3896 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>3896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M229.42,-150C229.42,-150 140.86,-150 140.86,-150 134.86,-150 128.86,-144 128.86,-138 128.86,-138 128.86,-98 128.86,-98 128.86,-92 134.86,-86 140.86,-86 140.86,-86 229.42,-86 229.42,-86 235.42,-86 241.42,-92 241.42,-98 241.42,-98 241.42,-138 241.42,-138 241.42,-144 235.42,-150 229.42,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.14\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">RM &lt;= 7.236</text>\n",
       "<text text-anchor=\"middle\" x=\"185.14\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 26</text>\n",
       "<text text-anchor=\"middle\" x=\"185.14\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 13.78</text>\n",
       "<text text-anchor=\"middle\" x=\"185.14\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 3.50</text>\n",
       "</g>\n",
       "<!-- 3895&#45;&gt;3896 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>3895&#45;&gt;3896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.26,-185.99C224.42,-177.32 217.99,-167.78 211.87,-158.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214.6,-156.48 206.11,-150.14 208.8,-160.39 214.6,-156.48\"/>\n",
       "</g>\n",
       "<!-- 3897 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3897</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M364.58,-150C364.58,-150 271.7,-150 271.7,-150 265.7,-150 259.7,-144 259.7,-138 259.7,-138 259.7,-98 259.7,-98 259.7,-92 265.7,-86 271.7,-86 271.7,-86 364.58,-86 364.58,-86 370.58,-86 376.58,-92 376.58,-98 376.58,-98 376.58,-138 376.58,-138 376.58,-144 370.58,-150 364.58,-150\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.14\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT &lt;= 5.1</text>\n",
       "<text text-anchor=\"middle\" x=\"318.14\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 378</text>\n",
       "<text text-anchor=\"middle\" x=\"318.14\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 16.86</text>\n",
       "<text text-anchor=\"middle\" x=\"318.14\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = &#45;0.24</text>\n",
       "</g>\n",
       "<!-- 3895&#45;&gt;3897 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3895&#45;&gt;3897</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M272.34,-185.99C278.33,-177.23 284.92,-167.58 291.2,-158.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.09,-160.37 296.85,-150.14 288.31,-156.42 294.09,-160.37\"/>\n",
       "</g>\n",
       "<!-- 3898 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3898</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.42,-50C100.42,-50 11.86,-50 11.86,-50 5.86,-50 -0.14,-44 -0.14,-38 -0.14,-38 -0.14,-12 -0.14,-12 -0.14,-6 5.86,0 11.86,0 11.86,0 100.42,0 100.42,0 106.42,0 112.42,-6 112.42,-12 112.42,-12 112.42,-38 112.42,-38 112.42,-44 106.42,-50 100.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 20</text>\n",
       "<text text-anchor=\"middle\" x=\"56.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 10.60</text>\n",
       "<text text-anchor=\"middle\" x=\"56.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 4.50</text>\n",
       "</g>\n",
       "<!-- 3896&#45;&gt;3898 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3896&#45;&gt;3898</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.1,-85.94C127.32,-76.22 112.15,-65.51 98.49,-55.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.39,-52.93 90.2,-50.03 96.36,-58.65 100.39,-52.93\"/>\n",
       "</g>\n",
       "<!-- 3899 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3899</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.42,-50C230.42,-50 141.86,-50 141.86,-50 135.86,-50 129.86,-44 129.86,-38 129.86,-38 129.86,-12 129.86,-12 129.86,-6 135.86,0 141.86,0 141.86,0 230.42,0 230.42,0 236.42,0 242.42,-6 242.42,-12 242.42,-12 242.42,-38 242.42,-38 242.42,-44 236.42,-50 230.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 6</text>\n",
       "<text text-anchor=\"middle\" x=\"186.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 9.83</text>\n",
       "<text text-anchor=\"middle\" x=\"186.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 0.15</text>\n",
       "</g>\n",
       "<!-- 3896&#45;&gt;3899 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3896&#45;&gt;3899</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.48,-85.94C185.57,-77.68 185.67,-68.72 185.76,-60.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.26,-60.07 185.87,-50.03 182.26,-59.99 189.26,-60.07\"/>\n",
       "</g>\n",
       "<!-- 3900 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>3900</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M361.42,-50C361.42,-50 272.86,-50 272.86,-50 266.86,-50 260.86,-44 260.86,-38 260.86,-38 260.86,-12 260.86,-12 260.86,-6 266.86,0 272.86,0 272.86,0 361.42,0 361.42,0 367.42,0 373.42,-6 373.42,-12 373.42,-12 373.42,-38 373.42,-38 373.42,-44 367.42,-50 361.42,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"317.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 51</text>\n",
       "<text text-anchor=\"middle\" x=\"317.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 21.60</text>\n",
       "<text text-anchor=\"middle\" x=\"317.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = 1.92</text>\n",
       "</g>\n",
       "<!-- 3897&#45;&gt;3900 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3897&#45;&gt;3900</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M317.8,-85.94C317.71,-77.68 317.61,-68.72 317.52,-60.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"321.01,-59.99 317.4,-50.03 314.01,-60.07 321.01,-59.99\"/>\n",
       "</g>\n",
       "<!-- 3901 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>3901</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M496.58,-50C496.58,-50 403.7,-50 403.7,-50 397.7,-50 391.7,-44 391.7,-38 391.7,-38 391.7,-12 391.7,-12 391.7,-6 397.7,0 403.7,0 403.7,0 496.58,0 496.58,0 502.58,0 508.58,-6 508.58,-12 508.58,-12 508.58,-38 508.58,-38 508.58,-44 502.58,-50 496.58,-50\"/>\n",
       "<text text-anchor=\"middle\" x=\"450.14\" y=\"-34.8\" font-family=\"Times,serif\" font-size=\"14.00\">Samples = 327</text>\n",
       "<text text-anchor=\"middle\" x=\"450.14\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">Impurity = 15.28</text>\n",
       "<text text-anchor=\"middle\" x=\"450.14\" y=\"-6.8\" font-family=\"Times,serif\" font-size=\"14.00\">Average y = &#45;0.58</text>\n",
       "</g>\n",
       "<!-- 3897&#45;&gt;3901 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>3897&#45;&gt;3901</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M363.2,-85.94C377.3,-76.22 392.82,-65.51 406.8,-55.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"409.04,-58.59 415.28,-50.03 405.06,-52.82 409.04,-58.59\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1324952d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = boston_df.values[:,:-1]\n",
    "y = boston_df.values[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "boston_gbdt = GradientBoostedDecisionTree(\n",
    "    is_classifier=False,\n",
    "    n_classes=1,\n",
    "    n_trees=20,\n",
    "    max_depth=2)\n",
    "boston_gbdt.fit(X_train, y_train)\n",
    "\n",
    "boston_feature_names = boston_data['feature_names']\n",
    "boston_gbdt.render(9,0,feature_names=boston_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gradient boosted decision tree accuracy on Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (R2 score) = 71.64%\n"
     ]
    }
   ],
   "source": [
    "y_pred = boston_gbdt.predict(X_test)\n",
    "test_acc = r2_score(y_test, y_pred)\n",
    "print(f'Test accuracy (R2 score) = {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
